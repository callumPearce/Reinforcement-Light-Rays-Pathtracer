\documentclass[../dissertation.tex]{subfiles}

\begin{document}

\chapter{Conclusions}
\label{chap:conclusion}

The objectives set out by this thesis were to learn the incident radiance function over the continuous set of locations in a scene. Then to design an algorithm which used the  approximation for importance sampling directions to continue light paths in as part of Monte Carlo path tracing, in order to reduce image noise. This algorithm then needs to be compared against an existing state of the art method which does the same, however approximates the incident radiance function by instead discretizing the continuous set of possible locations in the scene. With the new algorithm designed and evaluated, the question of is learning the incident radiance over the continuous set of possible locations in the scene beneficial for Monte Carlo path tracing compared to existing approaches in any way? This chapter describes the projects achievement of these goals and how this contributes to the field of computer graphics and efficient numerical integration. As well as  a discussion on the future research opportunities which have arisen following the conclusions made.

\section{Wider Motivation and the Problem}

Monte Carlo path tracing is a method used in Computer Graphics capable of producing photo-realistic images. Traditionally, it is known to trade off rendering time for the superior image quality it can produce, however increasing compute power, innovative algorithms and data-structures, and post processing have accelerated the speed of the algorithm by a significant amount. This combined with the lack of set-up required to render any arbitrary scene has led to a large resurgence in the algorithms use in industry, leading to even more research. In fact, a larger goal held across industry at this point is to achieve real-time rendering for methods like path tracing which accurately simulate light transport, but more work is required to make this a reality.\\

As suggested in the name, Monte Carlo path tracing uses Monte Carlo integration to ultimately approximate the true pixel value for all pixels in an image to render a scene, where a sample is a single light path. Therefore, importance sampling light paths for Monte Carlo path tracing has the potential to significantly improve every pixels colour estimate within the image, while using the same number of SPP. The task of finding correct information to importance sample light path directions with is a difficult one, and there have been many different approaches for doing so. NVIDIA's technique for the Expected Sarsa path tracer introduced the application of reinforcement learning for approximating the incident radiance function, as well as a new path tracing algorithm to use this approximation for importance sampling light path directions to great success. Their tabular TD-learning approach along with others required a discretization of locations in the scene to approximate the incident radiance function, whereas the scene in actual fact contains an infinite number of discrete positions. This raised the question that is it possible to learn the incident radiance function over the continuous set of location in the scene using a function approximation instead? Also, is this advantageous to Monte Carlo path tracing in any way? Finally, is neural network based importance sampling a promising technique for problems requiring Monte Carlo integration in general?

\section{Summary of Contributions}

We have introduced an ANN loss function based on Deep Q-learning \cite{}, along with an ANN architecture for learning the incident radiance function $L_i(x, \omega)$ for an arbitrary scene. A novel part of the introduced architecture is instead of passing a single position to the ANN as input, all vertices converted into a coordinate system relative to the point are passed in. Without this, the ANN was not able to learn the incident radiance function for even the most simplest scenes. To make use of the proposed ANN and loss function, a new path tracing algorithm known as the Neural-Q path tracer has been developed. Neural-Q uses the learning rule to efficiently train the ANN online during the rendering process. The approximation of the incident radiance function is then used to importance sample directions to continue light paths in, and to calculate probability density function needed for Monte Carlo path tracing. As more light paths are sampled in the rendering process, the accuracy of the approximated incident radiance function improves, leading to more efficient importance sampling of directions to continue light paths in. In turn, the variance in the approximation of pixel values during Monte Carlo path tracing using the same number of SPP is reduced. Directly reducing image noise. 

We have also introduced and compared the Expected Sarsa path tracer to our Neural-Q path tracer, as the Neural-Q path tracer approximates the incident radiance over the continuous set of possible positions in the scene, whereas the Expected Sarsa discretizes this set into discrete locations. In doing so, we were able to test the two algorithms against one another and found that the transition from the discrete to continuous set of positions in the scene brought various advantages. These include; improved incident radiance function approximation for scenes with more complex geometry, leading to noise reduction in images produced by Monte Carlo path tracing, Improved memory scaling for rendering scenes with more complex geometry, and simpler hyperparemeters for tuning. The Expected Sarsa algorithms applicability for rendering in industry is limited by the memory required to store the Q-table used to approximate the incident radiance function for complex scenes. Whereas, the Neural-Q algorithm is limited by the time taken to evaluate the forward pass on an ANN, this conclusion was also reached by two studies published during the execution of this thesis \cite{}.

\section{Discussion}

In this section the key conclusions made by our work are summarised. 

\subsubsection*{Reducing noise in Monte Carlo path tracing renders}
The results presented so far for the newly developed Neural-Q algorithm are promising, as it is able to significantly reduce noise for scenes rendered by Monte Carlo path tracing using the same number of SPP. Furthermore, it is able to reduce image noise more compared to the state of the art Expected Sarsa path tracer for scenes with more complex geometry by a significant margin, whilst using a smaller, constant amount of memory. Tuning the decaying $\epsilon$-greedy strategy used by Neural-Q for the scenes tested was found to be far easier and quicker process compared to deciding the amount of memory the Expected Sarsa algorithm requires to significantly reduce image noise. These factors combined with  Neural-Q's ability to efficiently train online to progressively reduce image noise across accumulated rendered frames for a scene, make it an interesting addition to the field of computer graphics. However, it is important to realise that newly proposed rendering algorithms should be tested for a broad range of scenes, especially those with extremely high numbers of polygons to fully assess which ones they apply best to. The Neural-Q algorithm and ANN architecture proposed is no exception to this. While I have shown it is able to perform well for a selection of different scenes, each one exhibiting different properties, scenes used in film and game production are yet to be experimented with for both the Neural-Q and Expected Sarsa path tracers.

\subsubsection*{Improving the approximation of the incident radiance function}
The reasoning for the reduction in noise was found to be the more accurate approximation made by the Neural-Q algorithm for the incident radiance function $L_i(x, \omega)$. This conclusion was reached by both further analysing the noise present in images rendered by the Expected Sarsa method and visualising the incident radiance distribution on three different points in a simple scene. Expected Sarsa poorer performance for renders was found to be a by-product of the more inaccurate incident radiance function aproximation made compared to that of the Neural-Q path tracers. This was related back to the theory of Monte Carlo importance sampling, which also revealed why there were a large number of fireflies present in the Expected Sarsa renders. 

\subsubsection*{Memory versus compute time}
While both the Neural-Q and Expected Sarsa path tracers are able to significantly reduce noise in images rendered by Monte Carlo path tracing, they both come with costs. The Expected Sarsa was found to be limited in its ability to reduce noise for scenes by the amount of memory available in the underlying the hardware the algorithm was running on to store the Q-table for approximating the incident radiance function. The real concern from this was that more complex scenes required more memory to receive a significant reduction in image noise, meaning scenes used in the production of films may be require an extremely large amount of memory which may not be able to be supported.

On the other hand, the Neural-Q algorithms requirement of frequently evaluating forward passes on an ANN for tracing every light path is very computationally expensive. In fact so much so, new purpose built hardware offered for quick inference on neural networks per thread on a GPU will likely be required to have any hope of using Neural-Q in industry. This conclusion was also shared by two other studies which were published during the execution of this thesis \cite{}, where ANN based importance sampling is currently limited in its ability to replace other methods by its severe computational cost. 

\subsubsection*{Neural Importance Sampling}
The benefits which are received by using ANNs for sampling directions to continue light paths in for Monte Carlo path tracing are now clear. However, not only does this present an addition to the field of computer graphics, but also presents a clear application of using neural networks for importance sampling in Monte Carlo methods, otherwise known as neural importance sampling. Meaning, other methods which require Monte Carlo integration to numerically solve integrals may benefit from an improvement in efficiency by adapting the approach outlined here to the problem. In fact, this is a hot topic in computer graphics right now where leaders in the computer graphics industry including Disney and NVIDIA are making serious progress in the application of neural importance sampling to Monte Carlo rendering techniques. But, to the best of our knowledge, the Neural-Q algorithm and ANN architecture described is the first which uses a shallow ANN to significantly reduce noise in images produced by Monte Carlo rendering.

Assessing the data collected in recent investigations, it seems as though approximating the incident radiance using Monte Carlo integration with neural importance sampling, rather than using an ANNs approximation of the incident radiance function to directly infer a pixels colour value has superior performance in terms of the image quality \cite{}. Hence, it is an area which we are likely to see significant more amount of research in within the near future.

\section{Future Work}

With the introduction of the new loss function and Neural-Q path tracer, as well as recent studies on neural importance sampling published during the execution of this thesis, many areas of future research have opened up. In this section we present the most interesting areas which we believe will lead to answers for the most important question currently surround neural importance sampling techniques for Monte Carlo path tracing. 

\subsubsection{Scaling up Neural-Q}

We consider this to be most important area to further extend the research completed by this thesis. There are three areas we propose that the Neural-Q algorithm should be scaled up:

\begin{enumerate}
\item \textbf{Testing high polygon count scenes} - For the Neural-Q path tracer to be adopted by industry it needs to be tested on scenes with potentially millions of polygons. If Neural-Q is able to continue to significantly reduce the noise in rendered images of these scenes, then it is only a question of how to speed the algorithm up, such that it is competitive with existing techniques in industry. However, we believe it is more likely further development will have to be done on the ANN architecture used by Neural-Q to learn the incident radiance function for a very complex scene. This is due to the complexity of the ANN architecture described in \cite{} to model scenes with complex geometry. However, our idea of using the vertices converted into a coordinate system centred around the input position may alleviate the need for this complexity.

\item \textbf{Accelerating Neural-Q on optimised hardware} - The experiments conducted in this thesis for the Neural-Q algorithm used an NVIDIA 1070Ti GPU, which is not even the latest series of commercial GPUs released by NVIDIA with the introduction of their new 20 series GPUs. Therefore, we believe it is necessary to test Neural-Q algorithm on a variety of available graphics cards, prioritising those with NVIDIA Tensor Cores to hopefully significantly speed up inference and hence the entire Neural-Q algorithm. Note, these tests should also be run on a more optimised path tracing engine then the one developed for this project, including common features such as bounding boxes and practices like aligned memory accesses. Comparisons should then be made against existing methods for importance sampling in Monte Carlo path tracing such as the Expected Sarsa algorithm and many others.

\item \textbf{Integrating Neural-Q into a production path tracer} - This point relies on the two previous ones being completed and receiving satisfactory data that proves Neural-Q can be applied to industry for rendering. By integrating Neural-Q to a production renderer, the algorithm can be assessed on rendering films and whether artists find it helpful or taxing on their work. This will also check if the algorithm combines well with post processing used in production renderer pipelines. For example image denoising by ANNs allowing the use of far less samples in Monte Carlo path tracing to begin with.
\end{enumerate}

\subsubsection{Investigating the relationship between scene geometry and ANN requirements}

From the results of the Neural-Q algorithm outperforming the Expected Sarsa method for scenes with more complex geometry, yet comparable or worse for those with simpler geometry, it is clear more research must be done to investigate how ANN architecture relates to the accuracy of the approximated incident radiance function for a scene. This is no easy task due to the variety of options to explore provided by the large amount work in the field of deep learning. Potentially assessing other function approximators such as Gaussian Processes for learning this function instead may potentially be profitable. However, we believe due to the abundance of data available in by continually sampling light paths, ANNs are likely to produce the best results. 

\subsubsection{Neural-Q versus Disney's Neural Path Guider}

The only other work to our knowledge which uses ANNs for importance sampling light path directions in Monte Carlo path tracing is that of Disney Research which was published during the execution of this thesis. As previously described the ANN framework they introduced known as NPG also learns online, hence the Neural-Q algorithm and NPG can be tested against one another for the performance in various aspects, similar to the approach in \ref{chap:critical_evaluation}. We believe NPG is likely to be more successful in reducing noise for an arbitrary scene, however it will be slower in doing so due to the smaller ANN architecture introduced by Neural-Q. We believe this is an important area to assess as there are no studies published regarding the comparison of multiple neural importance sampling schemes to  the best of our knowledge.

\subsubsection{Further investigate Deep Reinforcement learning techniques}

The field of Deep Reinforcement learning also has plenty to offer to Neural-Q. We believe we have only scratched the literature with what may be possible for applying deep reinforcement learning to neural importance sampling. Other frameworks such double deep q-learning (DDQN) offer more efficient and stable learning, whereas deep Expected Sarsa better translates to the rendering equation leading to a loss function which will train the ANN to better approximate the incident radiance function for a scene.


\begin{comment}

{\bf A compulsory chapter,     of roughly $5$ pages} 
\vspace{1cm} 

\noindent
The concluding chapter of a dissertation is often underutilised because it 
is too often left too close to the deadline: it is important to allocation
enough attention.  Ideally, the chapter will consist of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'' or ``users found feature Z of my software difficult to use,
      which is obvious in hindsight but not during at design stage; to 
      resolve this, I could clearly apply the technique of Smith [7]'').
\end{enumerate}

\subsection{Plan}
\begin{enumerate}

\item Summarise contributions to solving problem:
\begin{enumerate}
\item Extended temporal difference learning for the Incident Radiance function to the continuous spatial domain 

\item Proved it is possible for a 2 layer ANN to learn an accurate approximation of the Incident Radiance Function for a variety of scenes by the development of the new Neural-Q path tracing algorithm

\item Presented the weakness of Expected Sarsa when learning the incident radiance distribution on a point compared to the Neural-Q's smoother and more accurate approximation.

\item Presented that the Neural-Q algorithm is better able to reduce image noise for certain scenes compared to the Expected Sarsa algorithm proposed by NVIDIA using a smaller amount of memory and simpler hyperparameter tuning. Whilst sacrificing compute speed, limiting the algorithms current applicability to industry.
\end{enumerate}

\item Discussion of main findings and drawbacks:

\item Future work:

\begin{itemize}
\item If DQN does not work well provide some further analysis on potential other alternatives which could be used.

\item Future Work: Policy learning to model continuous action \& state space

\item DDQN and other deep reinforcement learning strategies
\end{itemize}
\end{enumerate}

\end{comment}

\end{document}