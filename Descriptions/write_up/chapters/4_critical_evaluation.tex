\documentclass[../dissertation.tex]{subfiles}

\begin{document}

\chapter{Comparing the Expected Sarsa and Neural-Q  Path Tracers}

\label{chap:evaluation}

Recall, that our main goal is to find out whether learning the incident radiance function for the continuous set of locations in a scene is more advantageous for importance sampling in path tracing, compared to learning the function for a discrete set of locations. To the best of our knowledge, there are no other studies which have attempted to make this assessment. Therefore, we have decided to analyse both the Expected Sarsa and Neural-Q path tracers in the following areas which we believe to show the most important differences:

\begin{itemize}
\item Assess the noise reduction and the type of noise which remains in the rendered images.
\item Assess the accuracy of the incident radiance approximation for scenes.
\item Find the number episodes required for learning to approximately converge.
\item Analyse the memory usage and compute time.
\item Assess the ease of setting hyperparameters for rendering an arbitrary scene.
\end{itemize}

\section{Experimental Setup}

We built a path tracing renderer from scratch which supported algorithms \ref{alg:forward_path_tracing}, \ref{alg:expected_sarsa_pathtracer}, \ref{alg:neural_q_pathtracer} and was used to produce all rendered results seen in this thesis. The rendering engine used only OpenGL Mathematics library \cite{glm} for various operations that are common in the rendering pipeline, SDL2 \cite{sdl2} for displaying rendered images, and the Dynet neural network library \cite{dynet} for the Neural-Q path tracer implementation. Algorithms \ref{alg:forward_path_tracing}, \ref{alg:expected_sarsa_pathtracer}, \ref{alg:neural_q_pathtracer} were all accelerated on an NVIDIA GPU by using the CUDA Toolkit \cite{cuda} to retrieve experimental results in an acceptable time. The reasoning for choosing Dynet over more commonly used neural networks libraries such as Tensorflow \cite{tensorflow2015-whitepaper}, was due to its ability to be easily compiled with the CUDA \verb|nvcc| compiler and it had a well documented C++ API. This was a requirement for the Neural-Q path tracer, as tracing light paths, ANN inference, and ANN training are all performed on a NVIDIA GPU via C++ API calls. All results were produced using a machine with an Intel i5-8600K CPU, NVIDIA 1070Ti GPU and 16GB of RAM installed.

We developed four different scenes using Maya \cite{maya} and created a custom object importer to import the scenes into our path tracing engine. A reference image of all four scenes is given in the very right column in figure \ref{fig:mape_results_grid}, which have been rendered with 4096 sampled SPP using the default path tracing algorithm \ref{alg:forward_path_tracing}. They are known as reference images as they have minimal visible noise, meaning the MC approximations for each pixel's colour value have approximately converged. Due to path tracing accurately modelling physical light transport (see section \ref{sec:mc_pathtracing}), these images are the ideal case for which all other path tracing algorithms should aim to produce as closely as possible, with the smallest number of SPP possible. Each rendered image from any of the techniques has been rendered at a resolution of 720X720.

\section{Assessing the reduction in image Noise for Monte Carlo Path Tracing}

Importance sampling techniques are used in path tracing to ultimately reduce image noise using the same number of SPP. Therefore, one of the most important points to assess is if our Neural-Q path tracer is able to reduce image noise to the same extent as the Expected Sarsa path tracer when using the same number of SPP. 

\subsection{Quantifying the Reduction in Image Noise}

To quantify the amount of noise within images rendered by a default forward path tracer, the Expected Sarsa path tracer, and the Neural-Q path tracer, we will use the Mean Absolute Percentage Error (MAPE) given in equation \ref{eq:mape} \cite{muller2018neural}.

\begin{equation}
\label{eq:mape}
M = \frac{1}{P} \sum_{i=0}^{P-1} \left| \frac{A_i - F_i}{A_i} \right|
\end{equation}

\noindent
Where:
\begin{conditions}
P & The total number of pixels in the image\\
A_i & The $i$th pixel value in the reference image\\
F_i & The $i$th pixel value in the image whose noise is being quantified\\
\end{conditions}

The MAPE value can then be used to quantify the average difference between pixel values from the reference image to each image rendered by the three path tracing algorithms with the same number of SPP. Therefore, a lower MAPE score is more desirable for a rendered image, as it means the image has a lower amount of noise. The MAPE score for images rendered by the three different path tracing algorithms for four different scenes  is shown in \ref{fig:mape_results_grid}. Each one of the four different scenes have been designed to exhibit different properties which affect light transport simulation. These can be summarised as:

\begin{itemize}
\item \textbf{Shelter} - A large scene which includes three different area lights, each with their own blockers. This makes it difficult to learn the contribution of radiance from each of the three sources to a given point in the scene. The scene is a sealed room, meaning all light paths will intersect with an area light as the number of reflections tend to infinity.

\item \textbf{Cornell Box} - A ubiquitous scene to test the accuracy of a rendering techniques approximation of global illumination. The scene is small and includes two light blockers. The wall in the directions of the camera is missing, meaning there is a void which causes light paths to potentially never intersect with an area light.

\item \textbf{Complex Pillars} - A scene which poses a true challenge to estimating the incident radiance on any point, due to the number of light blockers. The scene is sealed and the left, right, bottom, and top walls are all made to absorb most of the light incident on them. Meaning, light paths must navigate through the pillars to illuminate the back wall and the pillars themselves.

\item \textbf{Door Room} - The door room is a sealed scene which is almost entirely lit by indirect illumination. Light paths must reach the light before their contribution to the final image becomes negligible. 
\end{itemize}

From the MAPE score calculated for each rendered image of the four scenes using the three different path tracers, it is clear that the default forward path tracer in algorithm \ref{alg:forward_path_tracing} is inferior for a fixed number of SPP, compared to the path tracers which leverage the power of importance sampling in light path construction. Meaning, the Expected Sarsa and Neural-Q algorithms have both learnt an approximation of the incident radiance distribution function $L_i(x, \omega)$ for each scene, such that it is closer to normalizing the numerator in the Monte Carlo integration computed by path tracing (see equation \ref{eq:rendering_eq_monte_carlo}), compared to a uniform PDF. This causes a reduction in variance for the estimate of a pixel's colour value by using the two path tracers, leading to a lower amount of image noise within the renders displayed in figure \ref{fig:mape_results_grid}.

The MAPE scores for the Neural-Q path tracer are significantly lower for the \verb|Shelter| and \verb|Complex Pillars| scenes compared to the Expected Sarsa algorithms, and a tie is essentially reached for the \verb|Door Room| scene. Whereas, the Expected Sarsa clearly beats the Neural-Q path tracer for MAPE score on the \verb|Cornell Box|. Based on these MAPE results, the Neural-Q path tracer outperforms the Expected Sarsa path tracer for reducing image noise in renders of larger scenes with more complex geometry and a greater number of area light sources, while using less memory. The memory usage of the methods is discussed in detail in section \ref{sec:mem_to_comp}, where the actual amount of memory used to produce each of the rendered images from both the Neural-Q and Expected Sarsa method are given in table \ref{tab:memory_usage}. 

For smaller scenes with less complex geometry and a lower number of area light sources, the Expected Sarsa algorithm performs comparably well if not better. Our hypothesis behind why this is, is that the reduction in noise is largely due to the Neural-Q's ability to generalize the approximation of the incident radiance distribution at any position in the scene, and that this is more important for larger and more complex scenes. We will discuss this in more detail within sections \ref{sec:close_inspec_pixels} and \ref{sec:convergence_learning_incident}.

\begin{figure}[hbtp]
\begin{center}
\includegraphics[width=0.85\textwidth]{images/noise_reduction.png}    
\end{center}
\caption{A comparison of the default forward path tracer, Expected Sarsa path tracer, and the Neural-Q path tracer rendered image noise for four different rendered scenes. All images were rendered with 128 SPP, apart from the reference images in the very right column which use 4096 SPP. The score under each column in an image row corresponds to the MAPE score for each  path tracing algorithm for the particular scene.The Neural-Q and Expected Sarsa algorithms both used $144$ discrete directions in each discretized hemisphere to estimate the incident radiance distribution at a given point. The Neural-Q path tracer used the ANN architecture described in \ref{sec:ann_architecture} for all fours scenes with a decaying $\epsilon$-greedy policy starting at $\epsilon =1$, with a decay of $\delta = 0.05$ every epoch. The Expected Sarsa path tracer used just enough Irradiance Volumes (which varied depending on the scene) to facilitate a significant reduction in image noise in all four renders, such that it was competitive with the Neural-Q path tracer. See appendix \ref{appx:images} for the full images produced by all three path tracers.}
\label{fig:mape_results_grid}
\end{figure}

\pagebreak

\subsection{A Closer Inspection of Pixel Error Values}
\label{sec:close_inspec_pixels}

By visual observation of the rendered images \ref{fig:mape_results_grid}, the type of noise present in the Expected Sarsa and Neural-Q path tracer's renders are quite different. In particular, the noise resulting from the Expected Sarsa path tracer are pixels with very high RGB values compared to that of their neighbours, which are commonly referred to as 'fireflies'  \cite{christensen2016path}. Whereas the noise present in the Neural-Q renders is more subtle, however its general presence can be seen when comparing its renders to that of the reference images. In order to investigate this further, we have provided histograms in figure \ref{fig:histogram_errors} for the frequency of average RGB pixel error values of the rendered image compared to the reference image. Essentially, the greater the average RGB error for a particular pixel, the higher the amount of noise present in the pixel's colour estimate as a result of the path tracing algorithm used. Note, the histograms use a $\log$ scale, as the vast majority of pixel values when using 128 SPP for both algorithms have a low error, so the $\log$ scale allows us to compare the trend of high error estimates within the image. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.99\textwidth]{images/noise_diff.png}    
\end{center}
\caption{Histograms for the average RGB pixel error values for all four rendered scenes using both the Expected Sarsa and the Neural-Q path tracers. Where the average RGB pixel error value is the average difference in all RGB colour channels between a pixel in the reference image, and the corresponding pixel in the image rendered by either the Expected Sarsa or Neural-Q path tracers. The max average RGB pixel error value is $255$, which corresponds to the case where the reference images pixels value is $(255,255,255)$ whereas the rendered images was $(0,0,0)$ or vice versa. The histograms are calculated using the rendered images presented in figure \ref{fig:mape_results_grid}.}
\label{fig:histogram_errors}
\end{figure}

By comparing the tail end of the two rows of histograms, the reasoning behind the fireflies within the Expected Sarsa renders becomes clear. That is, on average the number of high average RGB error pixels is greater for the Expected Sarsa path tracer compared to the Neural-Q path tracer, which are the direct cause of fireflies. This validates the visual observation of fireflies in the renders shown in \ref{fig:mape_results_grid}. For example, the \verb|Door Room| scene render from the Expected Sarsa path tracer has many bright white and red pixels compared to the reference images, which corresponds to the high number of the large error values within the histogram.

The question now is, why are there more pixels with high error values in the Expected Sarsa path tracer renders compared to those produced by Neural-Q? To answer this question, first recall from section \ref{sec:importance_sampling} the closer the PDF comes to normalizing the function being integrated, the lower the variance in the Monte Carlo integral approximation. However, if the PDF used has a different shape to the function, it can actually increase the variance in the Monte Carlo approximation compared to using a uniform PDF. To relate this concept to learning the incident radiance function $L_i(x, \omega)$ for importance sampling in path tracing, if the error in the approximated incident radiance function is high for a scene, the noise in the approximated pixel values will be higher. This is because a normalised version of the approximated incident radiance function for a point is used to importance sample a direction to continue a light path in. More specifically, the incident radiance distribution at the intersection point of a light path is used to importance sample a direction to continue the light path in. Recall, for Expected Sarsa, this incident radiance distribution is calculated by normalizing $Q(x ,\omega_k)$ $\forall k = 1, ..., m$. Whereas for the Neural-Q it is calculated by normalizing $\hat{q_\theta}(x, \omega_k)$ $\forall k = 1,..., m$. Therefore, a comparison needs to be made between the learned incident radiance functions $Q(x ,\omega_k)$ and $\hat{q_\theta}(x, \omega_k)$  to understand why the Expected Sarsa and Neural-Q pixel error values are different.

\begin{figure}[h]
\centering
\minipage{0.31\textwidth}
  \includegraphics[width=\textwidth]{images/renders/true_distribution.png}   
  \subcaption{Ground Truth}\label{fig:true_incident_distribution}
\endminipage\hspace{1em}
\minipage{0.31\textwidth}
  \includegraphics[width=\textwidth]{images/renders/expected_sarsa_visualisation.png}   
  \subcaption{Expected Sarsa}\label{fig:exp_sarsa_distribution}
\endminipage\hspace{1em}
\minipage{0.31\textwidth}
  \includegraphics[width=\textwidth]{images/renders/neural_q_visualisation.png}
  \subcaption{Neural-Q}\label{fig:neural_q_distribution}
\endminipage
\caption{Visualisations of the incident radiance distribution for three different positions in a scene. The incident radiance is represented by placing an adaptive quadrature at each point with 144 different sectors, each representing an incident direction which the incident radiance has been calculated at. The positions are in front of a single large area light within a simple scene. (a) presents the ground truth incident radiance distribution, which has been calculated by firing 4096 sampled light paths per sector and averaging their incident radiance approximation. (b) The approximated incident radiance distribution for the three positions produced by the Expected Sarsa method when trained 1024 SPP, and (c) is the same but for the Neural-Q method trained with 1024 SPP. Due to the single large area light in front of the point, most of the incident radiance on the points will come from directions spanning from the area light on the left.}
\label{fig:distribution_visualisation}
\end{figure}

As previously explained incident radiance function $L_i(x, \omega)$ is a 5-dimensional function as $x \in \mathbb{R}^3$, $\omega \in \mathbb{R}^2$, making it difficult to visualize. Therefore, in figure \ref{fig:distribution_visualisation} a visual representation of the true incident radiance distribution, along with the Expected Sarsa and Neural-Q approximated distributions for three points in the scene are given. The scene is simple, it has no light blockers and there is a single area light from which the majority of incident radiance on the three points comes from. Meaning, it clearly presents the differences in the shape of the radiance distribution formed by normalising the incident radiance values approximated by the two methods at a point. The Neural-Q's approximation is clearly closer to true radiance distribution in figure \ref{fig:true_incident_distribution}, when compared to the Expected Sarsa method. The Expected Sarsa visualisation shows that the learned distribution gives far too much weighting to certain individual directions, making its approximation noticeably inaccurate, even for the simple scene shown. The Neural-Q approximated incident radiance distribution is not perfect, it gives slightly less weighting then it should do to directions facing the light. However it has clearly converged on which directions contribute the most incident radiance.

\begin{figure}[!htb]
\centering
\minipage{0.33\textwidth}
\begin{tikzpicture}
	\begin{axis}[every axis plot post/.append style={
	  mark=none,domain=-2:3,samples=50,smooth},
	  axis lines = left,
	  tick style={draw=none},
	  xticklabels={},
	  yticklabels={},
	    % All plots: from -2:2, 50 samples, smooth, no marks
	  axis x line*=bottom, % no box around the plot, only x and y axis
	  axis y line*=left, % the * suppresses the arrow tips
	  enlargelimits=upper,
	  xlabel = $\omega_i$,
	  ylabel = $p(\omega_i)$
	  ] % extend the axes a bit to the right and top
	  \addplot [
	  color=red,
	  ]{gauss(0.5,0.8)};
	  \addplot [
	  color=blue,
	  ]{gauss(0.5,0.5)};
	\end{axis}
\end{tikzpicture}
  \subcaption{Expected Sarsa}\label{fig:dist_expected_sarsa}
\endminipage
\minipage{0.33\textwidth}
    \begin{tikzpicture}
	\begin{axis}[every axis plot post/.append style={
	  mark=none,domain=-2:3,samples=50,smooth},
	  axis lines = left,
	  tick style={draw=none},
	  xticklabels={},
	  yticklabels={},
	    % All plots: from -2:2, 50 samples, smooth, no marks
	  axis x line*=bottom, % no box around the plot, only x and y axis
	  axis y line*=left, % the * suppresses the arrow tips
	  enlargelimits=upper,
	  xlabel = $\omega_i$,
	  ylabel = $p(\omega_i)$
	  ] % extend the axes a bit to the right and top
	  \addplot [
	  color=red,
	  ]{gauss(0.5,0.8)};
	  \addplot [
	  color=blue,
	  ]{gauss(0.5,1.0)};
	\end{axis}
	\end{tikzpicture}  
   \subcaption{Neural-Q}\label{fig:dist_neural_q}
\endminipage
\caption{An illustration of the incident radiance distribution for a given point directly in front of the light source shown in figure \ref{fig:distribution_visualisation} for both the Expected Sarsa and Neural-Q methods. Where $\omega_i \in \Omega$ are discrete incident angles on the point, and  $p(\omega_i)$ is the probability density function over incident radiance distribution evaluated at angle $\omega_i$. The red line represents the true incident radiance distribution on the point, the blue represents the corresponding methods approximation of the incident radiance distribution on the point.}
\label{fig:dist_2_methods}
\end{figure}

To give a better a conceptual understanding of why Expected Sarsa is less accurate compared to Neural-Q for the setting in figure \ref{fig:distribution_visualisation}, an illustration in figure \ref{fig:dist_2_methods} of an example incident radiance distribution for an intersection point in front of the area light is given, whereby normalising the incidence radiance prediction over all angles in a hemisphere $\Omega$ around an intersection point, a PDF over the predicted incident radiance on the intersection point is formed. The PDF for the Expected Sarsa approach is leptokurtic, meaning it contains an excess of extreme values causing it to incorrectly model the true radiance distribution for the point in the scene. Whereas the more mesokurtic curve for the Neural-Q path tracer more accurately models the true underlying radiance distribution for the point, however it is not perfect hence some noise is still present in rendered images in \ref{fig:mape_results_grid} for the Neural-Q path tracer.

 This inaccuracy by Expected Sarsa means for some incident angles $\omega_i$ which are not at the peak of the approximated distribution will likely have a significantly higher contribution of radiance to a point $x$ than the approximated radiance distribution states they will. This causes the evaulated PDF $pdf(x, \omega_i)$ to be small, and when used to divide the numerator in equation \ref{eq:rendering_eq_monte_carlo}, the approximated pixel colour value will be much higher than its true value. This presents a severe limitation to the Expected Sarsa's ability to reduce the noise in renders of a scene, as it adds more 'fireflies' to rendered images. As previously discussed in section \ref{sec:expected_sarsa_path_tracer}, the Expected Sarsa algorithm does not necessarily converge on the incident radiance \cite{dahm2017learning}, which is exactly what has been shown in figure \ref{fig:distribution_visualisation}.

\section{From a Discrete to Continuous State Space}

As a reminder, a state in the context of Neural-Q learning is an intersection location $x$ of a light path within the scene. An action is choosing a direction to continue the light path in from the intersection point. As discussed in section \ref{sec:deep_rl_motivation}, the Neural-Q algorithm uses an ANN as a function approximator for learning the incident radiance function ($L_i(x, \omega)$) where the approximation is denoted as $\hat{q_\theta}(x, \omega_k)$ for the discrete set of actions $\forall k = 1, ..., m$, over the continuous space of intersection positions $x \in \mathbb{R}^3$. Whereas Expected Sarsa uses a tabular approach denoted as $Q(x, \omega_k)$,  to approximate the incident radiance function at a discrete set of locations in the scene, using $\forall k = 1, ..., m$ discrete directions for each location. Therefore, as the the approximation of $L_i(x, \omega)$ by Neural-Q is made by an ANN, to find the estimated incident radiance for an unseen intersection location $x' \in \mathbb{R}^3$ in direction $\omega_k$, the ANN can use its current parameter values $\theta$  to generalize the incident radiance $\hat{q_\theta}(x', \omega_k) \approx L_i(x', \omega_k)$. Whereas the approach of Expected Sarsa is to find the closest Irradiance Volume to the point located at $\bar{x} \in \mathbb{R}^3$ and use its stored incident radiance estimate for the approximation, that is $Q(\bar{x}, \omega_k) \approx L_i(x', \omega_k)$. 

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/1_spp.png}   
\caption{Reference Images of the Door Room and Shelter scenes next to a 1SPP render using an Expected Sarsa path tracer and a Neural-Q path tracer, both trained for 100 epochs. The single sample for each pixel in the Expected Sarsa and Neural-Q renders is constructed by reflecting the light path in the direction of highest estimated radiance upon every intersection point, until the light is intersected with.}
\label{fig:1_spp_max_dir}
\end{figure}

We will no discuss how generalization for unseen intersection locations allows the ANN to learn a smooth approximation of the incident radiance for any point in the scene, compared to the tabular method used by Expected Sarsa. Observe figure \ref{fig:1_spp_max_dir}. As stated, a single light path is sampled through each pixel using the two different path tracing algorithms. The difference is that the trained Expected Sarsa and Neural-Q path tracers continue the light path at every intersection point in the direction of maximum predicted radiance. Therefore, the goal of \ref{fig:1_spp_max_dir} is to not reduce image noise in the renders of the two scenes at 1 SPP. Instead, it's to visualize for any point in the scene, the accuracy of the two methods approximation for the direction of the highest incident radiance.. 

For both renders shown in figure \ref{fig:1_spp_max_dir} produced by the trained Expected Sarsa path tracer, there are small distinct darker regions of the image. Each one of these small dark regions represents an Irradiance Volume which has not made a good approximation of which incident direction contributes the highest amount radiance. This creates small distinct areas where the estimation of incident radiance is poor, which is likely to lead to higher levels of image noise upon intersection with these locations during path tracing. On the other hand, a vastly different pattern can be seen for the Neural-Q algorithms renders, where boundaries between regions with a better approximation for the highest incident radiance direction are much smoother than that of the Expected Sarsa's. This is due to the generalization for incident radiance over the continuous set of locations in the scene, as no longer are discrete incident radiance values stored, but instead the parameter values $\theta$ of the network encode an approximation of the incident radiance function $L_i(x, \omega)$.

The question now is, does the generalisation over the state space made by Neural-Q actually help to reduce image noise compared to the discretized approach by Expected Sarsa? Unfortunately there is no clear answer to this question from the data we have collected. The reason being, the ANN generalization improves the approximation of $L_i(x, \omega)$ in both the \verb|Shelter| and \verb|Complex Pillars| scenes, whereas it performs equally as well for the \verb|Door Room| scene and worse for the \verb|Cornell Box|. So at best we can conclude the ANNs generalization can be more  beneficial for scenes with larger and more complex geometry whilst using less memory, as shown in table\ref{sec:mem_to_comp}. This is represented in figure\ref{fig:1_spp_max_dir}, where on average for the Neural-Q's render of the \verb|Shelter| scene shows a good approximation of the direction which the highest incident radiance is coming from, due to the smooth bright areas of the image. Whereas there are far more dark patches in the Expected Sarsa's render which represents incorrect approximations of the highest incident radiance direction. However, for smaller less complex scenes (\verb|Cornell Box| and \verb|Door Room| scenes), the ANN's approximation quality of incident radiance heavily changes across neighbouring regions in the scene. As a result, figure \ref{fig:mape_results_grid} shows Neural-Q's performance against the Expected Sarsa approach is slightly worse for reducing image noise for these scenes. In order to remedy the ANN's poor approximation in such situations, modifications in the ANN's architecture may be required. But this seems to be part of a wider problem which comes with a lack of research into how ANN architecture affects the quality of the approximated incident radiance function $L_i(x, \omega)$ for various types of scenes. Meaning more research is required before ANN's can be stably used to importance sample light paths in Monte Carlo path tracing to reduce image noise for any arbitrary scene. Luckily, some progress has been made in this area very recently, which we will discuss later in section \ref{sec:recent_advancements}.

\section{Convergence}
\label{sec:convergence_learning_incident}

Another important property which has not yet been discussed is the number of SPP required for the Expected Sarsa and Neural-Q path tracers to convergence on their approximation of the incident radiance function $L_i(x, \omega)$. To assess this, the number of \textit{zero contribution light paths} when rendering a single image has been collected, as well as the average number of reflections a light path undergoes before intersecting with a light source, known as the average \textit{path length}. These are both plotted against accumlated rendered images using 1 SPP (epoch). A zero contribution light path is a sampled light path which ends up contributing a negligible amount to a pixel value.

For a scene where a light path is guaranteed to intersect with an area light (sealed scene), as the average path length reduces, the number of zero contribution light paths also decreases. This is due to light paths with a shorter length generally contribute more to the rendered image because of the rendering equation. Following importance sampling for Monte Carlo integration, the lower the number of zero contribution light paths sampled in path tracing, the lower the image noise \cite{dahm2017learning}.  This is shown in figure \ref{fig:training_curves_archway}, where the average path length curves correlate with the number zero contribution light paths across accumulated frames for the \verb|Shelter| scene, leading to a reduction in noise from the first rendered image to the 100th rendered image for both algorithms using 1 SPP to render each image. Note, a similar pattern was seen for the \verb|Cornell Box|, \verb|Complex Pillars|, and \verb|Door Room| scenes.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\textwidth]{images/training_curves.png}    
\end{center}
\caption{Training curves for the average path length and number of zero contribution light paths when rendering the Shelter scene for 100 epochs, as well as the 1st and 100th rendered frames when using 1 SPP for both images. This is shown for both the Expected Sarsa and Neural-Q path tracers. An epoch represents one sampled light path through every pixel in the image. The average path length is the number reflections a light path takes before intersecting with a light source. A zero contribution light path is one which contributes almost zero colour to the final image.}
\label{fig:training_curves_archway}
\end{figure}

An interesting observation is how both the number of zero contribution light paths and the average path length for the Expected Sarsa algorithm fall far below that of the Neural-Q algorithm, yet the MAPE score for the \verb|Archway| scene is significantly higher for Expected Sarsa in figure \ref{fig:mape_results_grid}. This leads to the conclusion that the Expected Sarsa path tracer has the potential to further reduce the number of zero contribution light paths compared to the Neural-Q path tracer. However, this alone does not directly reduce image noise. Recall from the previous section \ref{sec:close_inspec_pixels} the leptokurtic shape of the incident radiance distribution approximated by the Expected Sarsa method for a given point in the room. Meaning, Expected Sarsa's approximated radiance distribution for this point did not accurately describe the true underlying radiance distribution. Therefore, the PDF used to normalise the MC prediction of a light paths colour estimate for Expected Sarsa caused a higher amount of image noise compared to that of the Neural-Q path tracer.

By observing the curves for the number of zero contribution light paths and average path length for both methods in figure \ref{fig:training_curves_archway}, it is clear that the Expected Sarsa approach is able to more quickly learn the incident radiance function $L_i(x, \omega)$ compared to the Neural-Q path tracer. However, the almost linear decrease seen for both curves using the Neural-Q method is due to the use of an $\epsilon$-greedy policy. As previously described in section \ref{sec:ann_architecture}, the $\epsilon$-greedy policy ensures the Neural-Q algorithm initially prioritises exploration ($\epsilon$ is high) to find the approximate value of $L_i(x,\omega)$ by uniformly at random sampling directions to continue light paths in. Every time all pixels in the image have had a light path sampled through them, the value of $\epsilon$ is decayed to favour exploitation more. This causes the Neural-Q path tracer to importance light path directions based on its current estimate of $L_i(x, \omega)$ more over time. As the decay was set to $\delta = 0.05$ and initially $\epsilon = 1$, at first the Neural-Q path tracer is limited to just randomly sampling directions to continue light paths in. But gradually over time, it favours importance sampling directions which are more likely to lead to a light source. This causes the close to linear downwards trend for the average path length and number of zero contribution light paths, as $\epsilon$ is decreased linearly. 

\section{From Memory Bound to Compute Bound}
\label{sec:mem_to_comp}

Computational resources utilized by the path tracing algorithms introduced is a very important talking point when considering their potential to be integrated into renders used in industry such as \cite{georgiev2018arnold, christensen2018renderman, hyperion}. In particular any bounds imposed on the algorithm from current available hardware are the most important to consider, as rendering algorithms should be designed to give as much power to artists as possible to create and render any arbitrary scene of their choice. A detailed investigation on both highly optimised implementations of the Expected Sarsa and Neural-Q path tracers performance using current available hardware warrants an investigation in itself. Instead for completeness, we present a high level analysis of how both memory usage and compute power limits the path tracers performance.

\subsection{Memory Usage}

The default forward path tracer is compute bound, meaning the only way to make the algorithm faster is by providing more compute power. For example, by parallelizing the rendering process using a GPU with a quicker processor clock speed. On the other hand, the performance of the Expected Sarsa algorithm for reducing image noise has been found to be bound by the amount of memory the algorithm has available to it from the  underlying hardware. This is due to the requirement of storing the Q-table in RAM which holds the approximated incident radiance values for a scene.

\begin{table}[h]
	\centering
	\begin{tabular}{lcc}
		\toprule
		& \multicolumn{2}{c}{Memory (MB)} \\ \cmidrule(lr){2-3}
		Scene & Expected Sarsa & Neural-Q \\
		\midrule
		Shelter & 271 & 30 \\
		Cornell Box & 44 & 30 \\
		Complex Pillars & 300 & 30 \\
		Door Room & 66 & 30 \\
		\bottomrule
	\end{tabular}
	\caption{Additional memory required in Megabytes (MB) to render each of the four scenes shown in figure \ref{fig:mape_results_grid} using both the Expected Sarsa and Neural-Q path tracers.}
	\label{tab:memory_usage}
\end{table}

\subsubsection{Expected Sarsa}

The size of memory used by the Q-table is defined by the number of Irradiance Volumes sampled across the scenes geometry, as well as the number incident radiance values stored for directions $\omega_k$ $\forall k = 1,...,m$  within each Irradiance Volume. We have found the higher the number of sampled Irradiance Volumes the more accurate the incident radiance approximation is for a given intersection point, as shown in figure \ref{fig:voronoi_difference}.  Intuitively this makes sense, as for the incident radiance function $L_i(x, \omega)$, the position $x \in \mathbb{R}^3$ can be infinitely many positions  in the scene. Therefore, the more Irradiance Volumes used to store approximated incident radiance values, the closer the approximation will be to the true function. In terms of the algorithm, the average distance from a light paths intersection point to the closest Irradiance Volume decreases as the number of Irradiance Volumes sampled in the scene increases. This becomes clear by observing figure \ref{fig:voronoi_difference}, where the Voronoi plot of the scene with a lower number of sampled Irradiance Volumes on average has a larger sector size. Meaning, on average the nearest neighbour search will return an Irradiance Volume further away from the intersection point, compared to when more Irradiance Volumes are sampled. Recall that a nearest neighbour search is used to estimate the incident radiance on a point based on the stored values in the closest Irradiance Volume, therefore the closer the Irradiance Volume used for this approximation, the more accurate the approximation of incident radiance on the point will be. However, notice for an over a 45X increase in Irradiance Volumes sampled in the \verb|Complex Pillars| scene, the difference in image noise by both observation and MAPE score is surprisingly small. From this, it seems that the number of Irradiance Volumes used in the Expected Sarsa method scales poorly with the reduction in image noise, even for a more complex scene such as the \verb|Complex Pillars| in figure \ref{fig:voronoi_difference}.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{images/less_volumes.png}   
\caption{The Complex Pillars scene rendered using two Expected Sarsa path tracers, both trained for 1000 SPP and the rendered images in the middle column are produced using 128 SPP. The first row presents the Expected Sarsa path tracer with 36029 Irradiance Volumes. The second row is the Expected Sarsa path tracer with only 780 Irradiance Volumes. The left column is a voronoi plot of the Irradiance Volumes, where each pixel is given a colour to represent which Irradiance Volume it is closest to. The MAPE score and render is given for both renders in the middle column. The right column presents two close-ups of the corresponding image in the middle column to give a visual comparison of the noise between the two images.}
\label{fig:voronoi_difference}
\end{figure}

Another problem with the relationship between increasing memory usage and reduction in image noise, is the cap on available memory imposed by the underlying hardware. If the memory available is too low, the algorithm may not be able to significantly reduce the amount of noise in rendered images. Now, this may not be a problem for most small scenes with a polygon count of around 100, as common commodity graphics cards such as the NVIDIA 1070Ti GPU used for our experiments come with 8GB of global memory (RAM) and NVIDIA claim they are able to fit the Q-table for these small scenes into only 2MB of memory to receive a significant reduction in noise \cite{dahm2017learning}. However, films commonly render scenes with hundred of thousands, or even millions of polygons, potentially making it impossible to store a Q-table large enough to significantly reduce image noise in memory. For example, in Transformers Revenge of the Fallen, the robot 'Devastator' was made out of 11,716,127 polygons \cite{devastator}. To make this  memory scaling issue clear, figure \ref{fig:incorrect_cornell} presents a render of the \verb|Cornell Box| scene which uses only 1MB of memory to store the Q-table.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{images/renders/sarsa_128_1Mb_incorrect.png}   
\caption{Cornell Box scene rendered using 1MB of memory to store the Irradiance Volumes for the Expected Sarsa path tracer. The large black artefacts and incorrect colouring of the red block is a result of using too few Irradiance Volumes to approximate the incident radiance function $L_i(x, \omega)$. Causing importance sampling to actually increase the variance in pixel value estimates, leading to an increase in overall image noise.}
\label{fig:incorrect_cornell}
\end{figure}

\subsubsection{Neural-Q}

From our experiments in figure \ref{fig:mape_results_grid}, the Neural-Q path tracer does not require significantly more memory to render large scenes. This is due to the Neural-Q path tracer only requiring more memory to store the incident radiance values for the current batch of light paths and to store ANN parameters $\theta$. The memory required to store the current batch of incident radiance values is controllable by the batch size parameter for the number of rays, therefore does not scale with the complexity of the scenes geometry. But a study from \cite{ren2013global} suggests more layers may be required in ANNs to approximate the incident radiance $L_i(x, \omega)$ for scenes with more complex geometry, leading to more memory being required to store more weight parameters of the network. However, further experimentation must be done to investigate the scaling of memory usage to scenes with thousands of vertices for the two rendering methods. 

The renders presented in figure \ref{fig:mape_results_grid} were produced using a constant ANN architecture as specified in section \ref{sec:ann_architecture}, which required only 30MB of memory to store the ANN as well as intermediate values computed during a forward pass (see table \ref{tab:memory_usage}). Whilst the Expected Sarsa path tracer required the rendered scenes with different geometry to have significantly different Q-table sizes to reduce rendered image noise. This is due to the ANNs ability to generalize the approximated incident radiance function over its 5-dimensional space. Where a single weight in the ANN has the capacity to contribute to the approximated incident radiance of many positions and directions in the scene. Hence, an ANN with 2 hidden layers was able to make a good approximation of the incident radiance for any $(x, \omega)$ for all tested scenes. The Q-table on the other hand holds an individual value for each incident radiance estimate $(x,\omega)$, therefore the storage requirements rose for the scenes rendered with more complex geometry. For example, the Q-table required for Expected Sarsa rendering of the large archway scene without any artefacts used 272MB of memory, 9X more than that of the Neural-Q path tracer.

\subsection{Parallel Processing}

To ensure high quality, high frame rate rendering of scenes, rendering algorithms have become very computationally demanding \cite{crockett1995parallel}. In order to facilitate this high level of performance, parallel computing must be used to distribute the computational workload. This pattern follows for both scanline renderers and ray-tracing renderers (which path tracing falls into) \cite{alerstam2008parallel, fatahalian2009data}. The default path tracing algorithm in particular is an embarrassingly parallel algorithm \cite{embarissingly_parallelizable}, as the task of finding the colour estimate of a single light path can be performed independently by a single process without any communication to other processes. A simple example of this is given in \cite{accelerated_ray_tracing}, whereby using a GPU the process of rendering an image using an algorithm which closely resembles path tracing is split among many cores. In industry path tracing algorithms take advantage of parallelism to an unprecedented scale, such as Disney's supercomputer for rendering full-length films using the Hyperion renderer \cite{hyperion}. Therefore, for any modified path tracing algorithm to gain industries attention it needs to be able to take full advantage of parallel computing hardware.

\subsubsection{Expected Sarsa}

The Expected Sarsa path tracer is no longer as trivial to parallelize as the default forward path tracing algorithm, due to the the first addition to the algorithm detailed in section \ref{sec:expected_sarsa_path_tracer}, where the incident radiance estimate for a given entry $Q(x,\omega_k)$ is updated for each intersection point $x$ and direction $\omega_k$ while constructing a light path. To parallelize the algorithm means to create an individual process to construct each light path to determine its colour estimate. However, the Q-table which stores the current approximation of all discrete incident radiance values in the scene is accessed by each thread, eaning, it is possible that two threads will attempt to write to the same location at the same time in the Q-table if they intersect near one another. This will cause the update rule provided in equation \ref{eq:mc_expected_sarsa_td_learning} to be applied incorrectly. To remedy this, the update rule can be placed within a critical code section \cite{raynal2012concurrent} to ensure only one thread at a time can calculate and set the updated incident radiance estimate. For our purposes, all code written for parallel execution used the CUDA Toolkit \cite{cuda}, where CUDA atomic operations were used to ensure only one thread at a time can calculate and apply the Q-value update. The critical section added a minor performance overhead to each thread in both waiting to and performing the update rule. However, it was negligible when using a Q-table near the size of any used to render the images in figure \ref{fig:mape_results_grid}. This is due to the number of Q-values stored, making it rare two threads attempt to write to the same address at the same time.

The significant performance penalty we found the Expected Sarsa approach adds to path tracing is the time taken to perform the nearest neighbour search for finding the closest Irradiance Volume to a given intersection point in the scene. Whilst we implemented a KD-Tree on the GPU in order to reduce the nearest neighbour computational complexity to $O( \log n)$, we found the Expected Sarsa path tracer ran nearly 10X times slower then that of the default path tracer. The cause of this slowdown was found to be in per thread memory read times to global CUDA memory \cite{cuda_c_guide}. 

As a quick overview of CUDA's thread hierarchy; an execution thread in CUDA is contained within a group of threads known as a thread block. Each thread block is executed by a symmetric multiprocessor (SM) placed in a GRID. Following this, the memory hierarchy consists of per-thread local memory, per-block shared memory, and global memory which any thread can read and write to at any time \cite{cuda_c_guide}. The issue with the Expected Sarsa path tracer, is that the Q-table needs to be globally accessible to any thread in the GPU at any time, as every thread will be constructing a light path and during its construction it may need to update any $Q(x, \omega)$ in the Q-table. This compounded with the fact that the Q-table is far too large to store in per-thread or per-block memory for any commodity GPU, means nearest neighbour lookup performs $O(\log n)$ global memory lookups. Each uncached global memory lookup has roughly 100X higher latency then that of per-thread or per-block memory lookups \cite{global_vs_shared}. Hence, the Expected Sarsa algorithm experiences a high performance penalty from idle time waiting for data to be returned from global memory to a core for processing. 

Interestingly, NVIDIA claims that the Expected Sarsa path tracer is able to render images with the same number of SPP by adding only a 20\% performance penalty on the default path tracers runtime  to significantly reduce image noise \cite{dahm2017learning}. However they do not specify the details of the machines that were running these experiments. Our implementation could be further optimised by ensuring aligned memory access and reducing the amount of data which must be retrieved during rendering. But we believe it is unlikely that any of these optimisations will be able to avoid the bottleneck of regularly querying global memory.

\subsubsection{Neural-Q}

As for the Neural-Q path tracer parallel execution is slightly different, whereby batches of light paths colour estimates are computed one at a time. Note, the batch size should be set to be large enough such that no processors are idle whilst computing a batch, whilst being small enough to fit the temporary approximated incident radiance values into memory. Recall, these temporary approximated incident radiance values are used for importance sampling directions to continue light paths in. 

Also, mini-batches which are the same size as the batch of light paths can be used in querying and training the ANN by stochastic gradient descent. A problem which added a significant performance penalty on our approach compared to that in \cite{muller2018neural}, is that there was no built in way to evaluate an ANN per-thread on the GPU using the Dynet framework. Only code from the CPU could call any of the Dynet API functions. Meaning, our implementation had to frequently send data produced from tracing a batch of light paths to their next intersection point (positions, normals, throughputs etcetera) from the GPU to the CPU, only to call a dynet function which moved the data back to the GPU again for evaluating the ANN. Within algorithm \ref{alg:neural_q_pathtracer} there are a total of $3$ calls to the Dynet API for evaluating the network for every batch of light ray paths, this means there is a total $6$ batch data transfers from the host device memory (memory that is directly accessible by the CPU) to GPU memory.

The latency added by this design flaw totaled to over a 100X slow down for our experiments with the Neural-Q path tracer. This was primarily due to the peak bandwidth available for transferring data between memory in the GPU is 144 GB/s, while for transferring data from the CPU to the GPU (and vice versa) is only 8 GB/s \cite{global_data_transfer}. Therefore, the 18x performance penalty received $6$ times with each batch computation, makes evaluating our implementation of the Neural-Q path tracer on render time not a fair test. However, the performance penalty received by methods which use neural network based importance sampling for  path tracing have found that the cost of evaluating the ANN without the data transfers from CPU to GPU (and vice verca) can still add a 10X peformance penalty, compared to other importance sampling methods in some cases \cite{muller2018neural, keller2019integral}. Clearly this needs to be alleviated before the neural network importance sampling methods are adopted for path tracing in industry. Answers to this problem may already lie in existing hardware, by leveraging the power of NVIDIA's TensorCores in Tesla GPUs \cite{tensor_cores}, which claim to have 40X higher performance than CPUs for inference on ANNs.

\section{Hyperparameters}

The final point of comparison between the Expected Sarsa and Neural-Q path tracers is the hyperparameters which must be configured for the algorithms. One of the enticing parts of using path tracing in industry, are its lack of hyperparameters \cite{georgiev2018arnold}. It turns out a lack of hyperparameters is very important to artists for rendering photo-realistic images of scenes, as they are able to iterate quicker in their development of a scene. For example, when developing a scene for a film, an artist will be constantly changing the geometry and will need a rendering algorithm which requires little configuration to quickly render a scene \cite{georgiev2018arnold}.

\subsubsection{Expected Sarsa}
The default path tracing algorithm (algorithm \ref{alg:forward_path_tracing}) only requires the number of SPP to be set for rendering. Along with the SPP, the Expected Sarsa path tracer requires the user to specify the amount of memory to use for the Q-table to approximate the incident radiance function \cite{dahm2017learning}. As described in section \ref{sec:mem_to_comp}, the higher the amount of memory given to the algorithm, the more Irradiance Volumes sampled around the scene. This leads to a more accurate approximation of the incident radiance function. Not only does this present the issue that for more complex scenes, more memory is required, but actually determining the minimal amount of memory required to significantly reduce the noise in rendered images without any artefacts  is a difficult task in itself. By artefacts, we mean large areas of the image which have a significant amount of noise, such as that seen in figure \ref{fig:incorrect_cornell}.

Our approach for determining the amount of memory a scene required was to instead determine the number of Irradiance Volumes to sample per-polygon in the scene. This was done by determining a surface area to Irradiance Volume ratio. For example, say the ratio was $1:10$, this means every $1 \text{cm}^2$ of surface area in the scene would have 10 Irradiance Volumes sampled upon it. This changes the parameter from arbitrarily guessing the memory required by the scene to a ratio, which we found takes less time to find the optimal value for in practice. However, the trial and error approach required to find the correct ratio is still present, but just to a lesser degree. Clearly finding the optimal memory usage for the discretization of the scene in the Expected Sarsa method is a time consuming process. In practice, it may take longer to find an optimal amount of memory to render a scene with, than it does to just use the default path tracer with a higher SPP to get rid of noise. Note, this is more of a problem for cases where memory usage is a concern, such as rendering scenes which require many textures to be stored in memory, or when the scene has a very high polygon count. Otherwise, a large amount of memory can just given to the algorithm and it should generally perform well.
 
\subsubsection{Neural-Q}
 The Neural-Q path tracer was shown to significantly reduce the noise in rendered images using a fixed amount of memory for the ANN architecture provided in figure \ref{fig:mape_results_grid}. Therefore, the parameter for memory provisioned by the algorithm does not need to be specified by the user. However, as discussed in section \ref{sec:mem_to_comp}, the ANN may require more layers to represent scenes with more complex geometry than those we have experimented with. For now, the Neural-Q algorithm only exposes two parameters; the beginning $\epsilon$ value and the decay rate $\delta$, both defining the $\epsilon$-greedy policy to follow. It is common practice to begin start with $\epsilon = 1$, as initially the reinforcement learning agent has no prior knowledge of which actions are more valuable to take \cite{mnih2013playing, van2016deep, mnih2015human}. The same goes for the Neural-Q path tracer, as initially the ANN has no prior knowledge of the incident radiance function. Meaning, $\epsilon = 1$ is a simple choice to make. 
 
 The less trivial parameter is the decay $\delta$ which should be high enough for the algorithm to quickly start exploiting its approximation of the incident radiance function $\hat{q_\theta}(x, \omega_k)$, by importance sampling light path directions. Yet low enough, such that the algorithm efficiently builds an accurate estimate of the incident radiance function. The choice of $\delta = 0.05$ for all renders presented in figure \ref{fig:mape_results_grid} was made as it fit the criteria specified for the four scenes experimented with. This is not to say this is the universal best decay rate for all scenes. It is likely scenes with a large number of blockers and complex geometry require more exploration, however this is something yet to be explored along with Neural-Q in general on scenes with a very large number of polygons.
 
The Neural-Q algorithm's parameters are clearly easier to configure for rendering an arbitrary scene, compared to the Expected Sarsa's. Having to manually decide on memory usage is no easy task and is far too dependent on the scene being rendered itself. Meaning, every change to geometry when building a scene using a production path tracer such as \cite{georgiev2018arnold, christensen2018renderman, hyperion} will require the difficult process of estimating the memory needed to render the scene without artefacts. This is something artists would definitely wish to avoid when designing a scene.

\section{Recent Advancements in Neural Importance Sampling for Monte Carlo Path Tracing}
\label{sec:recent_advancements}

%TODO using monte carlo approximation essentially smoothes the neural networks estimate rather then directly approximating the incident radiance -> Neural importance sampling shows great potential for other applications 

Along with Nvidia's paper which suggested a reinforcement learning approach for approximating the incident radiance at any point in a scene \cite{dahm2017learning}, other reinforcement learning based methods for learning light transport in a scene have been engineered \cite{muller2017practical, vorba2014line}. But none of these investigated the potential of deep reinforcement learning for this problem. However, recently neural networks for both the global and local sampling of light paths have had a large amount of research \cite{zheng2018learning, muller2018neural, keller2019integral, hermosilla2018deep}. Global light path sampling is where the optimal density of SPP, for every pixel is determined by a learning algorithm. Therefore, the reinforcement learning agent only determines where light paths should initially be sampled from. While this method does not reduce noise in the path tracing to the extent local importance sampling does, both \cite{muller2017practical, zheng2018learning} have applied this technique to reduce image noise with a much lower performance penalty compared to that of local importance sampling. We however, have been working on evaluating local light path importance sampling methods. Local light path importance sampling involves learning important directions to continue light paths in when they intersect with surfaces in the scene, in order to reduce image noise.

 The follow up paper to the one which introduced the Expected Sarsa path tracer \cite{dahm2017learning} is \cite{keller2019integral}, which was published towards the end of the execution of our work. Here, ANNs are experimented with for rendering in three different ways; for a given intersection point in the scene an ANN is trained to determine which light source should be used to compute the direct light incident on that intersection point, for approximating the visibility of an intersection point to all light sources, and for approximating the radiance at a given point in the scene to directly approximate a pixel's colour value using many small ANNs. The results are promising for all three use cases. However, unlike the Neural-Q path tracer, none of the methods estimate the incident radaince from a set of discrete directions around a point for importance sampling.
 
 Another paper published by Disney's Zurich research team during the implementation of our work in Febuary 2019 introduced \textit{neural importance sampling} \cite{muller2018neural}. Neural importance sampling samples a direction to continue a light path in and Disney call their algorithm for doing so Neural Path Guiding (NPG). This is currently the only other method which is an alternative for using neural networks to importance sample directions to continue light paths in, based on a learned approximation of the incident radiance function. An ANN framework for modelling complex high-dimensional densities known as Non-linear independent Component Estimation (NICE) \cite{dinh2014nice} is trained to learn the distribution of incident radiance at any given point in the scene. However, unlike our approach, the network takes in the intersection point $x \in \mathbb{R}^3$, the intersection point's surface normal and the outgoing direction $\omega_o$ of interest to directly evaluate $L_o(x, \omega_o)$. A one blob encoding is then applied to these inputs which improves the speed and performance of inference by the network to output a single direction $\omega$ to continue the light path in. The PDF over the hemisphere $\Omega$ of possible directions to sample from at an intersection point $x$, can also be evaluated at $\omega$ using their proposed ANN framework. This has the advantage over Neural-Q as it is able to learn the incident radiance function over both the continuous set of locations and directions in the scene, rather than discretizing the possible directions using the adaptive quadrature technique. However, the ANN framework they proposed is far more expensive to evaluate than that used by the Neural-Q path tracer, as it consists of a one-blob encoding, many fully connected layers, and a piecewise polynomial warp \cite{muller2018neural}. Whereas, the Neural-Q network in comparison only consists of two hidden layers, hence requires less memory for storing parameter values and evaluating the network compared to that of NPG. 

\begin{comment}

{\bf A topic-specific chapter, of roughly $15$ pages} 
\vspace{1cm} 

\noindent
This chapter is intended to evaluate what you did.  The content is highly 
topic-specific, but for many projects will have flavours of the following:

\begin{enumerate}
\item functional  testing, including analysis and explanation of failure 
      cases,
\item behavioural testing, often including analysis of any results that 
      draw some form of conclusion wrt. the aims and objectives,
      and
\item evaluation of options and decisions within the project, and/or a
      comparison with alternatives.
\end{enumerate}

\noindent
This chapter often acts to differentiate project quality: even if the work
completed is of a high technical quality, critical yet objective evaluation 
and comparison of the outcomes is crucial.  In essence, the reader wants to
learn something, so the worst examples amount to simple statements of fact 
(e.g., ``graph X shows the result is Y''); the best examples are analytical 
and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
contradicts [1], which may be because I use a different assumption'').  As 
such, both positive {\em and} negative outcomes are valid {\em if} presented 
in a suitable manner.

\subsection{Plan}

\textbf{Data to collect}
\begin{itemize}

\item Build 4 different scenes:

\begin{itemize}
\item Simple geometry, Indirectly illuminated scene: Here both reinforcement learning methods should perform excellently

\item Simple geometry, Directly illuminated scene: Here all methods should perform well

\item Complex geometry, Indirectly illuminated scene: Can both methods do this - will take a lot of training, deeper NN potentially

\item Complex geometry, Directly illuminated scene: Can both methods do this - will take a lot of training, deeper NN potentially
\end{itemize}

\item Number zero-contribution light paths/ light paths that do not intersect with a with a light after $n$ bounces therefore they become irrelevant for all methods with accumulated frames on the x-axis

\item Variance in points around the room to train network in order to make training batches as varied as possible (this is a weird one, essentially assessing the fact that we do not need a replay buffer).

\item eta-greedy constant for loss curve for training the network \& decaying eta-greedy policy graph for the loss as well

\item Visual representation of Q-values being higher in directions near light source: Map q-values to hemispheres in the scene and get a close up, clearly indicating its ability to sample in the correct direction

\item 1 SPP, 16 SPP, 32 SPP, 64 SPP, 128 SPP, 256 SPP for all three methods on 4 different scenes to evaluate their effectiveness: Assessing accuracy of global illumination approximation

\item Limitations: Number of angles which can accurately be learned by the network, accuracy  needs to be compared with expected SARSA approach for a single radiance volume at a given point in the scene. Size of the scene which can be learnt accurately.

\end{itemize}

\textbf{Preliminary}
\begin{enumerate}
\item Exploration vs Exploitation for both techniques, exploration can yield to better results plus exploitation does not accurately simulate light, relate to the rendering equation and how light works in the physical world.

\item Show for about 4 different scenes the results for a $n$ different numbers of samples; the images, average path length, number of light paths which actually contribute to the image which are sampled between all techniques. I will have to analyse which reduces the number of zero contribution paths the most, but also still assess if the image is photo-realistic.

\item Also analyse default Q-learnings ability on top of expected SARSA

\item Justify reasoning for choosing to analyse Q-Learning, Expected SARSA and DQN (because they have good results for other cases and TD learning fits the online learning procedure)

\item Assess the number of parameters required, configuration is important for these algorithms, if it is very difficult to get right, then the time spent configuring may not be worth it compared to actually rendering the image. E.g. default path-tracing there are not other parameters apart from the number of samples per pixel, expected SARSA requires the user to specify the memory which is allowed to be used by the program, this requires careful consideration, as well as the threshold the distribution cannot fall below, the deep Q-learning algorithm requires less config but potentially different neural network architectures should be investigated to further reduce the number of zero-contribution light paths. 

\item Ease of implementation 

\item Parallelisability of each algorithm, path-tracing is far easier to parallelise as it requires minimal memory accesses by the program to infer pixel values, as opposed to expected SARSA which requires many. Deep-q learning has more customizability in terms of parallelizing (needs more research)

\item Memory usage: Path-tracing is minimal, Expected SARSA is unbounded, Deep Q-Learning is bounded by the size of the neural network, but the memory it requires is still significant (needs more research)

\item DQN vs Expected Sarsa: Do not have to wait for an iteration to begin
 importance sampling on the newly learned Q values for a given point, 
 neural network is continually trained and inferred from. Continuous state 
 space vs discretized required for storage in expected SARSA.
\end{enumerate}

\end{comment}

\end{document}