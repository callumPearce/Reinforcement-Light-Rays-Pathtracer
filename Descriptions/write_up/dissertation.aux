\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\citation{latexbook1}
\citation{latexbook2}
\providecommand \oddpage@label [2]{}
\citation{sutton2011reinforcement}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.0.1}Plan}{xix}{subsection.0.0.1}}
\citation{christensen2016path}
\citation{keller2016path}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Contextual Background}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:context}{{1}{1}{Contextual Background}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Path Tracing for Light Transport Simulation}{1}{section.1.1}}
\newlabel{sec:conceptual_path_trace}{{1.1}{1}{Path Tracing for Light Transport Simulation}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An illustration of path tracing, where three light paths are traced from from the camera through a pixel, to the light source in a simple 3D scene.\relax }}{1}{figure.caption.12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:path_tracing_overview}{{1.1}{1}{An illustration of path tracing, where three light paths are traced from from the camera through a pixel, to the light source in a simple 3D scene.\relax }{figure.caption.12}{}}
\citation{krivanek2014recent}
\citation{jensen1996global}
\citation{keller2016path}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Two renders of the Cornell Box, where the left is directly illuminated and the right is globally illuminated.\relax }}{2}{figure.caption.13}}
\newlabel{fig:direct_and_global}{{1.2}{2}{Two renders of the Cornell Box, where the left is directly illuminated and the right is globally illuminated.\relax }{figure.caption.13}{}}
\citation{sutton2011reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Two renders of a simple room using 16 sampled light paths per pixel. Where one does not use importance sampling in the construction of light paths (left), and the other does so based on a reinforcement learning rule (right). A clear reduction in image noise can be seen.\relax }}{3}{figure.caption.14}}
\newlabel{fig:noise_reduction_simple_room}{{1.3}{3}{Two renders of a simple room using 16 sampled light paths per pixel. Where one does not use importance sampling in the construction of light paths (left), and the other does so based on a reinforcement learning rule (right). A clear reduction in image noise can be seen.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Temporal Difference Learning for Importance Sampling Ray Directions}{3}{section.1.2}}
\newlabel{sec:td_learn_for_importance}{{1.2}{3}{Temporal Difference Learning for Importance Sampling Ray Directions}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is Temporal Difference learning?}{3}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Temporal Difference learning methods for Efficient Light Transport Simulation}{3}{subsection.1.2.2}}
\citation{ramamoorthi2012theory}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Why use Temporal Difference Learning for Importance Sampling?}{4}{subsection.1.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces An illustration of a light blocker for an importance sampling scheme which does not consider visibility. Each arrow represents a possible direction the light path will be reflected in. Clearly the reflected light path is likely to hit the blocker increasing the likelihood of it becoming a zero-contribution light path.\relax }}{4}{figure.caption.15}}
\newlabel{fig:blocker}{{1.4}{4}{An illustration of a light blocker for an importance sampling scheme which does not consider visibility. Each arrow represents a possible direction the light path will be reflected in. Clearly the reflected light path is likely to hit the blocker increasing the likelihood of it becoming a zero-contribution light path.\relax }{figure.caption.15}{}}
\citation{pan2006virtual}
\citation{bloomberg.com}
\citation{nvidia_turing_architecture_whitepaper_2018}
\citation{bako2017kernel}
\citation{chaitanya2017interactive}
\citation{christensen2018renderman}
\citation{georgiev2018arnold}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Motivation}{5}{section.1.3}}
\newlabel{sec:motivation}{{1.3}{5}{Motivation}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Real time Rendering using Accurate Light Transport Simulation}{5}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Recent Developments}{5}{subsection.1.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Challenges and Objectives}{5}{section.1.4}}
\citation{morokoff1995quasi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Technical Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:technical}{{2}{7}{Technical Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Monte Carlo Integration and Importance Sampling}{7}{section.2.1}}
\newlabel{sec:monte_carlo}{{2.1}{7}{Monte Carlo Integration and Importance Sampling}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Monte Carlo Integration}{7}{subsection.2.1.1}}
\newlabel{sec:monte_carlo_approx}{{2.1.1}{7}{Monte Carlo Integration}{subsection.2.1.1}{}}
\newlabel{eq:integral}{{2.1}{7}{Monte Carlo Integration}{equation.2.1.1}{}}
\newlabel{eq:monte_carlo}{{2.2}{7}{Monte Carlo Integration}{equation.2.1.2}{}}
\newlabel{eq:generalized_mc}{{2.3}{7}{Monte Carlo Integration}{equation.2.1.3}{}}
\citation{morokoff1995quasi}
\citation{scratchapixel_2015}
\newlabel{eq:unbiased}{{2.4}{8}{Monte Carlo Integration}{equation.2.1.4}{}}
\newlabel{eq:law_large_numbers}{{2.5}{8}{Monte Carlo Integration}{equation.2.1.5}{}}
\newlabel{eq:sample_variance}{{2.6}{8}{Monte Carlo Integration}{equation.2.1.6}{}}
\newlabel{eq:mc_error}{{2.7}{8}{Monte Carlo Integration}{equation.2.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Importance Sampling for Reducing Approximation Variance}{8}{subsection.2.1.2}}
\newlabel{sec:importance_smapling}{{2.1.2}{8}{Importance Sampling for Reducing Approximation Variance}{subsection.2.1.2}{}}
\newlabel{eq:constant_monte_carlo}{{2.8}{8}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.8}{}}
\newlabel{eq:constant_conversion}{{2.9}{8}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.9}{}}
\citation{kajiya1986rendering}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Constant Function  with a sample point\relax }}{9}{figure.caption.16}}
\newlabel{fig:constant_function}{{2.1}{9}{Constant Function\\ with a sample point\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Non-linear Function  with a sample point\relax }}{9}{figure.caption.16}}
\newlabel{fig:non_lin_function}{{2.2}{9}{Non-linear Function\\ with a sample point\relax }{figure.caption.16}{}}
\newlabel{eq:solve_mc_integration}{{2.10}{9}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.10}{}}
\newlabel{fig:improtance_correct}{{2.3a}{9}{Importance sampling reducing variance\relax }{figure.caption.17}{}}
\newlabel{sub@fig:improtance_correct}{{a}{9}{Importance sampling reducing variance\relax }{figure.caption.17}{}}
\newlabel{fig:improtance_uniform}{{2.3b}{9}{Uniform\\ sampling\relax }{figure.caption.17}{}}
\newlabel{sub@fig:improtance_uniform}{{b}{9}{Uniform\\ sampling\relax }{figure.caption.17}{}}
\newlabel{fig:improtance_incorrect}{{2.3c}{9}{Importance sampling increasing variance\relax }{figure.caption.17}{}}
\newlabel{sub@fig:improtance_incorrect}{{c}{9}{Importance sampling increasing variance\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Graphical representation of a function $f(x)$ (red) and the corresponding probability density function $pdf(x)$ (blue) used in the Monte Carlo integration approximation for the integral of $f(x)$.\relax }}{9}{figure.caption.17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Monte Carlo Path Tracing}{9}{section.2.2}}
\newlabel{sec:mc_pathtracing}{{2.2}{9}{Monte Carlo Path Tracing}{section.2.2}{}}
\citation{dutre2004state}
\citation{glassner2014principles}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Rendering Equation}{10}{subsection.2.2.1}}
\newlabel{sec:rendering_equation}{{2.2.1}{10}{The Rendering Equation}{subsection.2.2.1}{}}
\newlabel{eq:rendering_equation}{{2.11}{10}{The Rendering Equation}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(h(x, \omega _i),-\omega _i) = L_i(y_i, -\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(y_i, -\omega _i)$ is identical to calculating the outgoing radiance $L_o(y_i, -\omega _i)$ as we assume no radiance is lost along a ray line, hence the $L_o$ is a recursive function. \relax }}{10}{figure.caption.18}}
\newlabel{fig:recursive_rendering}{{2.4}{10}{A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(h(x, \omega _i),-\omega _i) = L_i(y_i, -\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(y_i, -\omega _i)$ is identical to calculating the outgoing radiance $L_o(y_i, -\omega _i)$ as we assume no radiance is lost along a ray line, hence the $L_o$ is a recursive function. \relax }{figure.caption.18}{}}
\citation{normals}
\citation{glassner2014principles}
\newlabel{fig:diffuse_brdf}{{2.5a}{11}{Diffuse BRDF\relax }{figure.caption.19}{}}
\newlabel{sub@fig:diffuse_brdf}{{a}{11}{Diffuse BRDF\relax }{figure.caption.19}{}}
\newlabel{fig:specular_brdf}{{2.5b}{11}{Specular BRDF\relax }{figure.caption.19}{}}
\newlabel{sub@fig:specular_brdf}{{b}{11}{Specular BRDF\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A representation of both a diffuse surface and specular surface BRDF for a given angle of incidence $\omega '$. The surface point is located where all end of the arrows converge. The arrows indicate a subset of direction possible for the incident ray to be reflected in. All possible directions reflected directions for a ray are defined between the surface point and the line , for an incident direction $\omega '$. The further away a point is on the line, the more likely a ray is to reflected in a direction from the surface point to that point on the line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favour a small subset are of direction in the hemisphere surrounding the surface point.\relax }}{11}{figure.caption.19}}
\newlabel{fig:brdfs}{{2.5}{11}{A representation of both a diffuse surface and specular surface BRDF for a given angle of incidence $\omega '$. The surface point is located where all end of the arrows converge. The arrows indicate a subset of direction possible for the incident ray to be reflected in. All possible directions reflected directions for a ray are defined between the surface point and the line , for an incident direction $\omega '$. The further away a point is on the line, the more likely a ray is to reflected in a direction from the surface point to that point on the line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favour a small subset are of direction in the hemisphere surrounding the surface point.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two sculptures, one made from a diffuse material (left) and the other from a specular material.\relax }}{11}{figure.caption.20}}
\newlabel{fig:material_pics}{{2.6}{11}{Two sculptures, one made from a diffuse material (left) and the other from a specular material.\relax }{figure.caption.20}{}}
\citation{stanford_graphics}
\citation{christensen2016path}
\newlabel{eq:conservation_energy}{{2.12}{12}{The Rendering Equation}{equation.2.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Path Tracing}{12}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Path Tracing}{12}{section*.21}}
\newlabel{sec:monte_carlo_path_tracing}{{2.2.2}{12}{Monte Carlo Path Tracing}{section*.21}{}}
\newlabel{eq:rendering_eq_monte_carlo}{{2.13}{12}{Monte Carlo Path Tracing}{equation.2.2.13}{}}
\citation{bashford2012significance}
\citation{cline2008table}
\citation{pegoraro2008towards}
\citation{dahm2017learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces An indirectly illuminated scene from a default path tracer. The grid of image sections represent an increasing number of samples per pixel (SPP), beginning in the top left with 16 SPP, to the bottom right with 512 SPP. The full image on the right is a reference image with 4096 SPP where the Monte Carlo approximation has almost converged for pixel values.\relax }}{13}{figure.caption.22}}
\newlabel{fig:reduce_noise_spp_example}{{2.7}{13}{An indirectly illuminated scene from a default path tracer. The grid of image sections represent an increasing number of samples per pixel (SPP), beginning in the top left with 16 SPP, to the bottom right with 512 SPP. The full image on the right is a reference image with 4096 SPP where the Monte Carlo approximation has almost converged for pixel values.\relax }{figure.caption.22}{}}
\newlabel{alg:forward_path_tracing}{{1}{13}{Monte Carlo Path Tracing}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Forward Path Tracer. Given a camera position, scene geometry, this algorithm will render a single image by finding the colour estimate for each pixel using Monte Carlo path tracing. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{13}{algocf.1}}
\@writefile{toc}{\contentsline {subsubsection}{Importance Sampling in Path Tracing}{13}{section*.23}}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{Existing Methods for Importance Sampling}{14}{section*.24}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning and TD-Learning}{14}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Markov Decision Processes}{14}{subsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Markov Decision Process \cite  {sutton2011reinforcement}\relax }}{14}{figure.caption.25}}
\newlabel{fig:mdp}{{2.8}{14}{Markov Decision Process \cite {sutton2011reinforcement}\relax }{figure.caption.25}{}}
\citation{introToRL}
\citation{sutton2011reinforcement}
\citation{introToRL}
\citation{sutton2011reinforcement}
\newlabel{eq:markov_property}{{2.14}{15}{Markov Decision Processes}{equation.2.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Goals and Rewards}{15}{subsection.2.3.2}}
\newlabel{eq:return}{{2.15}{15}{Goals and Rewards}{equation.2.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Value Function and Optimality}{15}{subsection.2.3.3}}
\newlabel{sec:optimal_value}{{2.3.3}{15}{Value Function and Optimality}{subsection.2.3.3}{}}
\citation{sutton2011reinforcement}
\citation{model_free_prediction}
\citation{sutton2011reinforcement}
\citation{mdp_dynamic_prog}
\citation{sutton2011reinforcement}
\newlabel{eq:value_function}{{2.16}{16}{Value Function and Optimality}{equation.2.3.16}{}}
\newlabel{eq:optimal_value}{{2.17}{16}{Value Function and Optimality}{equation.2.3.17}{}}
\newlabel{eq:bellman_optimal}{{2.18}{16}{Value Function and Optimality}{equation.2.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Temporal Difference Learning}{16}{subsection.2.3.4}}
\newlabel{sec:td_learning}{{2.3.4}{16}{Temporal Difference Learning}{subsection.2.3.4}{}}
\citation{sutton2011reinforcement}
\citation{exploration_vs_exploitation}
\newlabel{eq:sarsa}{{2.19}{17}{Sarsa}{equation.2.3.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Q-Learning}{17}{section*.27}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{17}{section*.28}}
\newlabel{eq:expected_sarsa}{{2.22}{17}{Expected Sarsa}{equation.2.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Exploration vs Exploitation}{17}{subsection.2.3.5}}
\newlabel{sec:exploration_vs_exploitation}{{2.3.5}{17}{Exploration vs Exploitation}{subsection.2.3.5}{}}
\citation{sutton2011reinforcement}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Linking TD-Learning and Light Transport Simulation}{18}{section.2.4}}
\newlabel{sec:td_light_transport}{{2.4}{18}{Linking TD-Learning and Light Transport Simulation}{section.2.4}{}}
\newlabel{eq:sarsa_integral}{{2.25}{18}{Linking TD-Learning and Light Transport Simulation}{equation.2.4.25}{}}
\citation{greger1998irradiance}
\newlabel{eq:expected_sarsa_td_learning}{{2.26}{19}{Linking TD-Learning and Light Transport Simulation}{equation.2.4.26}{}}
\newlabel{eq:mc_expected_sarsa_td_learning}{{2.27}{19}{Linking TD-Learning and Light Transport Simulation}{equation.2.4.27}{}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\citation{greger1998irradiance}
\citation{shirley1994notes}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}TD-Learning and Deep Reinforcement Learning for Importance Sampling Light Paths}{21}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:td_deep_sampling}{{3}{21}{TD-Learning and Deep Reinforcement Learning for Importance Sampling Light Paths}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The expected Sarsa Path Tracer}{21}{section.3.1}}
\newlabel{sec:expecte_sarsa_path_tracer}{{3.1}{21}{The expected Sarsa Path Tracer}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Irradiance Volume}{21}{subsection.3.1.1}}
\citation{bentley1975multidimensional}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An Irradiance Volume. Each sector holds the incoming radiance $L_i(x,\omega _k)$, the more green a sector is the lower the stored radiance in that sector, the more red a sector is the higher the stored radiance in that sector. \relax }}{22}{figure.caption.30}}
\newlabel{fig:irradiance_volume}{{3.1}{22}{An Irradiance Volume. Each sector holds the incoming radiance $L_i(x,\omega _k)$, the more green a sector is the lower the stored radiance in that sector, the more red a sector is the higher the stored radiance in that sector. \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example of discretizing location in the scene into Irradiance Volume locations. The geometry mesh (a) is used to uniformly sample Irradiance volume positions. Image (b) shows a voronoi plot for the Irradiance Volumes in the scene, where each pixel is coloured to the represent its closest Irradiance Volume, so each sector of colour in (b) represents a different Irradiance Volume location. Finally (c) gives a render using the Expected Sarsa path tracer based on Algorithm \ref  {alg:expected_sarsa_pathtracer}.\relax }}{22}{figure.caption.31}}
\newlabel{fig:scene_discretization_example}{{3.2}{22}{An example of discretizing location in the scene into Irradiance Volume locations. The geometry mesh (a) is used to uniformly sample Irradiance volume positions. Image (b) shows a voronoi plot for the Irradiance Volumes in the scene, where each pixel is coloured to the represent its closest Irradiance Volume, so each sector of colour in (b) represents a different Irradiance Volume location. Finally (c) gives a render using the Expected Sarsa path tracer based on Algorithm \ref {alg:expected_sarsa_pathtracer}.\relax }{figure.caption.31}{}}
\citation{devroye2006nonuniform}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Expected Sarsa Path Tracing}{23}{subsection.3.1.2}}
\newlabel{sec:expected_sarsa_path_tracer}{{3.1.2}{23}{Expected Sarsa Path Tracing}{subsection.3.1.2}{}}
\newlabel{alg:expected_sarsa_pathtracer}{{2}{24}{Addition 3: Update Irradiance Volume Distributions}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Expected Sarsa path tracer pseudo code following Nvidia's method in \cite  {dahm2017learning}. Given a camera position, scene geometry, this algorithm will render a single image using a tabular Expected Sarsa approach to progressively reduce image noise. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{24}{algocf.2}}
\newlabel{eq:mc_expected_sarsa_pdf}{{3.1}{24}{Monte Carlo Integration}{equation.3.1.1}{}}
\citation{dahm2017learning}
\newlabel{fig:uniform_pdf}{{3.3a}{25}{Hemisphere with a uniform $pdf$\relax }{figure.caption.36}{}}
\newlabel{sub@fig:uniform_pdf}{{a}{25}{Hemisphere with a uniform $pdf$\relax }{figure.caption.36}{}}
\newlabel{fig:not_uniform_pdf}{{3.3b}{25}{Irradiance Volume with a non-uniform $pdf$\relax }{figure.caption.36}{}}
\newlabel{sub@fig:not_uniform_pdf}{{b}{25}{Irradiance Volume with a non-uniform $pdf$\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A 2 dimension view of a subset of values from two probability density functions ($pdf$). One for a unit hemisphere (left) with a uniform $pdf$. One for an Irradiance Volume (right) with non-uniform pdf. Where the arrows represent sampled directions and the values at the end are the evaluated $pdf$ values for each direction.\relax }}{25}{figure.caption.36}}
\newlabel{fig:pdfs}{{3.3}{25}{A 2 dimension view of a subset of values from two probability density functions ($pdf$). One for a unit hemisphere (left) with a uniform $pdf$. One for an Irradiance Volume (right) with non-uniform pdf. Where the arrows represent sampled directions and the values at the end are the evaluated $pdf$ values for each direction.\relax }{figure.caption.36}{}}
\newlabel{eq:decay_lr}{{3.2}{25}{Consistency}{equation.3.1.2}{}}
\citation{deep_rl_function_approx}
\citation{sutton2011reinforcement}
\citation{sutton1996generalization}
\citation{konidaris2011value}
\citation{uther1998tree}
\citation{lecun2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Neural-Q Path Tracer}{26}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to Deep Reinforcement Learning}{26}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Value Function Approximation}{26}{section*.38}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{26}{section*.39}}
\newlabel{eq:example_loss}{{3.3}{26}{Stochastic Gradient Descent}{equation.3.2.3}{}}
\citation{deep_rl_function_approx}
\citation{lillicrap2015continuous}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{Bootstrapping}{27}{section*.40}}
\newlabel{sec:bootstrapping}{{3.2.1}{27}{Bootstrapping}{section*.40}{}}
\newlabel{eq:td_error_q_learning}{{3.4}{27}{Bootstrapping}{equation.3.2.4}{}}
\newlabel{eq:loss_q_learning}{{3.5}{27}{Bootstrapping}{equation.3.2.5}{}}
\newlabel{eq:q_learning_derv_loss}{{3.6}{27}{Bootstrapping}{equation.3.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Deep Reinforcement Learning Motivation}{27}{subsection.3.2.2}}
\citation{mnih2013playing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Deep Q-learning for Light Transport}{28}{subsection.3.2.3}}
\newlabel{eq:neural_q_loss}{{3.7}{28}{Deep Q-learning for Light Transport}{equation.3.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Artificial Neural Network Architecture}{28}{subsection.3.2.4}}
\newlabel{sec:ann_architecture}{{3.2.4}{28}{Artificial Neural Network Architecture}{subsection.3.2.4}{}}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\citation{mnih2013playing}
\citation{nair2010rectified}
\citation{ren2013global}
\citation{sutton2011reinforcement}
\citation{deep_rl_function_approx}
\citation{lillicrap2015continuous}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Neural-Q Path Tracing Algorithm}{30}{subsection.3.2.5}}
\citation{dahm2017learning}
\citation{muller2017practical}
\citation{vorba2014line}
\citation{zheng2018learning}
\citation{muller2018neural}
\citation{keller2019integral}
\citation{hermosilla2018deep}
\citation{muller2017practical}
\citation{zheng2018learning}
\citation{dahm2017learning}
\citation{keller2019integral}
\newlabel{alg:neural_q_pathtracer}{{3}{32}{Addition 4: Decaying decaying $\epsilon $-\textit {greedy}}{algocfline.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Neural-Q forward path tracer. Given a camera position, scene geometry, epsilon and epsilon decay, this algorithm will render a single image using deep Q-learning loss to progressively reduce image noise. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{32}{algocf.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Recent Advancements in Neural Importance Sampling for Monte Carlo Path Tracing}{32}{subsection.3.2.6}}
\citation{muller2018neural}
\citation{dinh2014nice}
\citation{muller2018neural}
\citation{glm}
\citation{sdl2}
\citation{dynet}
\citation{cuda}
\citation{tensorflow2015-whitepaper}
\citation{maya}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Critical Evaluation}{35}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{4}{35}{Critical Evaluation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{35}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Assessing the reduction in image Noise for Monte Carlo Path Tracing}{35}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Quantifying the Reduction in Image Noise}{35}{subsection.4.2.1}}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{hyperion}
\newlabel{eq:mape}{{4.1}{36}{Quantifying the Reduction in Image Noise}{equation.4.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Convergence for Learning Incident Radiance}{36}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Hyper Parameter Tuning}{36}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Reusing Trained ANN for Different Scenes}{36}{subsection.4.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}From Memory Bound to Compute Bound}{36}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Memory Usage}{36}{subsection.4.3.1}}
\citation{dahm2017learning}
\citation{devastator}
\citation{ren2013global}
\citation{crockett1995parallel}
\citation{alerstam2008parallel}
\citation{fatahalian2009data}
\citation{embarissingly_parallelizable}
\citation{accelerated_ray_tracing}
\citation{hyperion}
\citation{raynal2012concurrent}
\citation{cuda}
\citation{cuda_c_guide}
\citation{global_vs_shared}
\citation{dahm2017learning}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{tensor_cores}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Parallel Processing}{38}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Neural-Q Design Discussion}{39}{section.4.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{41}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{41}{Conclusion}{chapter.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Plan}{41}{subsection.5.0.1}}
\bibdata{dissertation}
\bibcite{glm}{1}
\bibcite{sdl2}{2}
\bibcite{tensorflow2015-whitepaper}{3}
\bibcite{alerstam2008parallel}{4}
\bibcite{accelerated_ray_tracing}{5}
\bibcite{maya}{6}
\bibcite{bako2017kernel}{7}
\bibcite{bashford2012significance}{8}
\bibcite{bentley1975multidimensional}{9}
\bibcite{bloomberg.com}{10}
\bibcite{chaitanya2017interactive}{11}
\bibcite{christensen2018renderman}{12}
\bibcite{christensen2016path}{13}
\bibcite{cline2008table}{14}
\bibcite{crockett1995parallel}{15}
\bibcite{dahm2017learning}{16}
\bibcite{devroye2006nonuniform}{17}
\bibcite{dinh2014nice}{18}
\bibcite{dutre2004state}{19}
\bibcite{fatahalian2009data}{20}
\bibcite{embarissingly_parallelizable}{21}
\bibcite{georgiev2018arnold}{22}
\bibcite{glassner2014principles}{23}
\bibcite{greger1998irradiance}{24}
\bibcite{stanford_graphics}{25}
\bibcite{hermosilla2018deep}{26}
\bibcite{jensen1996global}{27}
\bibcite{kajiya1986rendering}{28}
\bibcite{keller2019integral}{29}
\bibcite{keller2016path}{30}
\bibcite{konidaris2011value}{31}
\bibcite{krivanek2014recent}{32}
\bibcite{lecun2015deep}{33}
\bibcite{lillicrap2015continuous}{34}
\bibcite{mnih2013playing}{35}
\bibcite{morokoff1995quasi}{36}
\bibcite{muller2017practical}{37}
\bibcite{muller2018neural}{38}
\bibcite{nair2010rectified}{39}
\bibcite{dynet}{40}
\bibcite{cuda}{41}
\bibcite{nvidia_turing_architecture_whitepaper_2018}{42}
\bibcite{pan2006virtual}{43}
\bibcite{pegoraro2008towards}{44}
\bibcite{ramamoorthi2012theory}{45}
\bibcite{ren2013global}{46}
\bibcite{scratchapixel_2015}{47}
\bibcite{shirley1994notes}{48}
\bibcite{hyperion}{49}
\bibcite{sutton1996generalization}{50}
\bibcite{sutton2011reinforcement}{51}
\bibcite{uther1998tree}{52}
\bibcite{exploration_vs_exploitation}{53}
\bibcite{deep_rl_function_approx}{54}
\bibcite{introToRL}{55}
\bibcite{mdp_dynamic_prog}{56}
\bibcite{model_free_prediction}{57}
\bibcite{vorba2014line}{58}
\bibcite{normals}{59}
\bibcite{zheng2018learning}{60}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}An Example Appendix}{47}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appx:example}{{A}{47}{An Example Appendix}{appendix.A}{}}
