\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\citation{latexbook1}
\citation{latexbook2}
\providecommand \oddpage@label [2]{}
\citation{dahm2017learning}
\citation{christensen2016path}
\citation{kajiya1986rendering}
\citation{keller2016path}
\citation{stanford_graphics}
\citation{krivanek2014recent}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:context}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Monte Carlo Path Tracing for Light Transport Simulation}{1}{section.1.1}}
\newlabel{sec:conceptual_path_trace}{{1.1}{1}{Monte Carlo Path Tracing for Light Transport Simulation}{section.1.1}{}}
\citation{dahm2017learning}
\citation{jensen1996global}
\citation{keller2016path}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An illustration of path tracing, where three light paths are traced from the camera through a pixel, to the light source in a simple 3D scene. The light paths are used to determine the pixel colours of the rendered image.\relax }}{2}{figure.caption.8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:path_tracing_overview}{{1.1}{2}{An illustration of path tracing, where three light paths are traced from the camera through a pixel, to the light source in a simple 3D scene. The light paths are used to determine the pixel colours of the rendered image.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Two renders of the Cornell Box, where the left is directly illuminated and the right is globally illuminated.\relax }}{2}{figure.caption.9}}
\newlabel{fig:direct_and_global}{{1.2}{2}{Two renders of the Cornell Box, where the left is directly illuminated and the right is globally illuminated.\relax }{figure.caption.9}{}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\citation{sutton2011reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Two renders of a simple room using 16 SPP. Where one does not use importance sampling in the construction of light paths (left), and the other does so based on a reinforcement learning rule \cite  {dahm2017learning} (right). A clear reduction in image noise can be seen by the use of importance sampling.\relax }}{3}{figure.caption.10}}
\newlabel{fig:noise_reduction_simple_room}{{1.3}{3}{Two renders of a simple room using 16 SPP. Where one does not use importance sampling in the construction of light paths (left), and the other does so based on a reinforcement learning rule \cite {dahm2017learning} (right). A clear reduction in image noise can be seen by the use of importance sampling.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Temporal Difference Learning for Importance Sampling Ray Directions}{3}{section.1.2}}
\newlabel{sec:td_learn_for_importance}{{1.2}{3}{Temporal Difference Learning for Importance Sampling Ray Directions}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is Temporal Difference learning?}{3}{subsection.1.2.1}}
\citation{dahm2017learning}
\citation{ramamoorthi2012theory}
\citation{dahm2017learning}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Temporal Difference learning methods for Efficient Light Transport Simulation}{4}{subsection.1.2.2}}
\newlabel{sec:temp_diff_for_light_transport}{{1.2.2}{4}{Temporal Difference learning methods for Efficient Light Transport Simulation}{subsection.1.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Why use Temporal Difference Learning for Importance Sampling?}{4}{subsection.1.2.3}}
\newlabel{sec:why_temp_diff}{{1.2.3}{4}{Why use Temporal Difference Learning for Importance Sampling?}{subsection.1.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces An illustration of a light blocker for an importance sampling scheme which does not consider visibility. Each arrow represents a possible direction the light path will be continued in. Clearly the reflected light path is likely to hit the blocker, reducing its contribution to the approximation of a pixel value.\relax }}{4}{figure.caption.11}}
\newlabel{fig:blocker}{{1.4}{4}{An illustration of a light blocker for an importance sampling scheme which does not consider visibility. Each arrow represents a possible direction the light path will be continued in. Clearly the reflected light path is likely to hit the blocker, reducing its contribution to the approximation of a pixel value.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}From Discrete to a Continuous State Space}{4}{subsection.1.2.4}}
\newlabel{sec:discrete_to_continuous_motivation}{{1.2.4}{4}{From Discrete to a Continuous State Space}{subsection.1.2.4}{}}
\citation{kajiya1986rendering}
\citation{goral1984modeling}
\citation{whitted2005improved}
\citation{jensen1996global}
\citation{wald2001state}
\citation{pan2006virtual}
\citation{bloomberg.com}
\citation{nvidia_turing_architecture_whitepaper_2018}
\citation{bako2017kernel}
\citation{chaitanya2017interactive}
\citation{christensen2018renderman}
\citation{georgiev2018arnold}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Motivation}{5}{section.1.3}}
\newlabel{sec:motivation}{{1.3}{5}{Motivation}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Real time Rendering using Accurate Light Transport Simulation}{5}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Recent Developments}{5}{subsection.1.3.2}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Challenges and Objectives}{6}{section.1.4}}
\citation{morokoff1995quasi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Technical Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:technical}{{2}{7}{Technical Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Monte Carlo Integration and Importance Sampling}{7}{section.2.1}}
\newlabel{sec:monte_carlo}{{2.1}{7}{Monte Carlo Integration and Importance Sampling}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Monte Carlo Integration}{7}{subsection.2.1.1}}
\newlabel{sec:monte_carlo_approx}{{2.1.1}{7}{Monte Carlo Integration}{subsection.2.1.1}{}}
\newlabel{eq:integral}{{2.1}{7}{Monte Carlo Integration}{equation.2.1.1}{}}
\newlabel{eq:monte_carlo}{{2.2}{7}{Monte Carlo Integration}{equation.2.1.2}{}}
\newlabel{eq:generalized_mc}{{2.3}{7}{Monte Carlo Integration}{equation.2.1.3}{}}
\citation{morokoff1995quasi}
\citation{scratchapixel_2015}
\newlabel{eq:unbiased}{{2.4}{8}{Monte Carlo Integration}{equation.2.1.4}{}}
\newlabel{eq:law_large_numbers}{{2.5}{8}{Monte Carlo Integration}{equation.2.1.5}{}}
\newlabel{eq:sample_variance}{{2.6}{8}{Monte Carlo Integration}{equation.2.1.6}{}}
\newlabel{eq:mc_error}{{2.7}{8}{Monte Carlo Integration}{equation.2.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Importance Sampling for Reducing Approximation Variance}{8}{subsection.2.1.2}}
\newlabel{sec:importance_smapling}{{2.1.2}{8}{Importance Sampling for Reducing Approximation Variance}{subsection.2.1.2}{}}
\newlabel{eq:constant_monte_carlo}{{2.8}{8}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.8}{}}
\newlabel{eq:constant_conversion}{{2.9}{8}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.9}{}}
\citation{kajiya1986rendering}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Constant Function  with a sample point\relax }}{9}{figure.caption.12}}
\newlabel{fig:constant_function}{{2.1}{9}{Constant Function\\ with a sample point\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Non-linear Function  with a sample point\relax }}{9}{figure.caption.12}}
\newlabel{fig:non_lin_function}{{2.2}{9}{Non-linear Function\\ with a sample point\relax }{figure.caption.12}{}}
\newlabel{eq:solve_mc_integration}{{2.10}{9}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.10}{}}
\newlabel{fig:improtance_correct}{{2.3a}{9}{Importance sampling reducing variance\relax }{figure.caption.13}{}}
\newlabel{sub@fig:improtance_correct}{{a}{9}{Importance sampling reducing variance\relax }{figure.caption.13}{}}
\newlabel{fig:improtance_uniform}{{2.3b}{9}{Uniform\\ sampling\relax }{figure.caption.13}{}}
\newlabel{sub@fig:improtance_uniform}{{b}{9}{Uniform\\ sampling\relax }{figure.caption.13}{}}
\newlabel{fig:improtance_incorrect}{{2.3c}{9}{Importance sampling increasing variance\relax }{figure.caption.13}{}}
\newlabel{sub@fig:improtance_incorrect}{{c}{9}{Importance sampling increasing variance\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Graphical representation of a function $f(x)$ (red) and the corresponding probability density function $pdf(x)$ (blue) used in the Monte Carlo integration approximation for the integral of $f(x)$.\relax }}{9}{figure.caption.13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Monte Carlo Path Tracing}{9}{section.2.2}}
\newlabel{sec:mc_pathtracing}{{2.2}{9}{Monte Carlo Path Tracing}{section.2.2}{}}
\citation{dutre2004state}
\citation{glassner2014principles}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Rendering Equation}{10}{subsection.2.2.1}}
\newlabel{sec:rendering_equation}{{2.2.1}{10}{The Rendering Equation}{subsection.2.2.1}{}}
\newlabel{eq:rendering_equation}{{2.11}{10}{The Rendering Equation}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(h(x, \omega _i),-\omega _i) = L_i(y_i, -\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(y_i, -\omega _i)$ is identical to calculating the outgoing radiance $L_o(y_i, -\omega _i)$ as we assume no radiance is lost along a ray line, hence the $L_o$ is a recursive function. \relax }}{10}{figure.caption.14}}
\newlabel{fig:recursive_rendering}{{2.4}{10}{A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(h(x, \omega _i),-\omega _i) = L_i(y_i, -\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(y_i, -\omega _i)$ is identical to calculating the outgoing radiance $L_o(y_i, -\omega _i)$ as we assume no radiance is lost along a ray line, hence the $L_o$ is a recursive function. \relax }{figure.caption.14}{}}
\citation{normals}
\citation{glassner2014principles}
\newlabel{fig:diffuse_brdf}{{2.5a}{11}{Diffuse BRDF\relax }{figure.caption.15}{}}
\newlabel{sub@fig:diffuse_brdf}{{a}{11}{Diffuse BRDF\relax }{figure.caption.15}{}}
\newlabel{fig:specular_brdf}{{2.5b}{11}{Specular BRDF\relax }{figure.caption.15}{}}
\newlabel{sub@fig:specular_brdf}{{b}{11}{Specular BRDF\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A representation of both a diffuse surface and specular surface BRDF for a given angle of incidence $\omega '$. The surface point is located where all end of the arrows converge. The arrows indicate a subset of direction possible for the incident ray to be reflected in. All possible directions reflected directions for a ray are defined between the surface point and the line , for an incident direction $\omega '$. The further away a point is on the line, the more likely a ray is to reflected in a direction from the surface point to that point on the line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favour a small subset are of direction in the hemisphere surrounding the surface point.\relax }}{11}{figure.caption.15}}
\newlabel{fig:brdfs}{{2.5}{11}{A representation of both a diffuse surface and specular surface BRDF for a given angle of incidence $\omega '$. The surface point is located where all end of the arrows converge. The arrows indicate a subset of direction possible for the incident ray to be reflected in. All possible directions reflected directions for a ray are defined between the surface point and the line , for an incident direction $\omega '$. The further away a point is on the line, the more likely a ray is to reflected in a direction from the surface point to that point on the line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favour a small subset are of direction in the hemisphere surrounding the surface point.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two sculptures, one made from a diffuse material (left) and the other from a specular material.\relax }}{11}{figure.caption.16}}
\newlabel{fig:material_pics}{{2.6}{11}{Two sculptures, one made from a diffuse material (left) and the other from a specular material.\relax }{figure.caption.16}{}}
\citation{stanford_graphics}
\citation{christensen2016path}
\newlabel{eq:conservation_energy}{{2.12}{12}{The Rendering Equation}{equation.2.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Path Tracing}{12}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Path Tracing}{12}{section*.17}}
\newlabel{sec:monte_carlo_path_tracing}{{2.2.2}{12}{Monte Carlo Path Tracing}{section*.17}{}}
\newlabel{eq:rendering_eq_monte_carlo}{{2.13}{12}{Monte Carlo Path Tracing}{equation.2.2.13}{}}
\citation{bashford2012significance}
\citation{cline2008table}
\citation{pegoraro2008towards}
\citation{dahm2017learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces An indirectly illuminated scene from a default path tracer. The grid of image sections represent an increasing number of samples per pixel (SPP), beginning in the top left with 16 SPP, to the bottom right with 512 SPP. The full image on the right is a reference image with 4096 SPP where the Monte Carlo approximation has almost converged for pixel values.\relax }}{13}{figure.caption.18}}
\newlabel{fig:reduce_noise_spp_example}{{2.7}{13}{An indirectly illuminated scene from a default path tracer. The grid of image sections represent an increasing number of samples per pixel (SPP), beginning in the top left with 16 SPP, to the bottom right with 512 SPP. The full image on the right is a reference image with 4096 SPP where the Monte Carlo approximation has almost converged for pixel values.\relax }{figure.caption.18}{}}
\newlabel{alg:forward_path_tracing}{{1}{13}{Monte Carlo Path Tracing}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Forward Path Tracer. Given a camera position, scene geometry, this algorithm will render a single image by finding the colour estimate for each pixel using Monte Carlo path tracing. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{13}{algocf.1}}
\@writefile{toc}{\contentsline {subsubsection}{Importance Sampling in Path Tracing}{13}{section*.19}}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{Existing Methods for Importance Sampling}{14}{section*.20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning and TD-Learning}{14}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Markov Decision Processes}{14}{subsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Markov Decision Process \cite  {sutton2011reinforcement}\relax }}{14}{figure.caption.21}}
\newlabel{fig:mdp}{{2.8}{14}{Markov Decision Process \cite {sutton2011reinforcement}\relax }{figure.caption.21}{}}
\citation{introToRL}
\citation{sutton2011reinforcement}
\citation{introToRL}
\citation{sutton2011reinforcement}
\newlabel{eq:markov_property}{{2.14}{15}{Markov Decision Processes}{equation.2.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Goals and Rewards}{15}{subsection.2.3.2}}
\newlabel{eq:return}{{2.15}{15}{Goals and Rewards}{equation.2.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Value Function and Optimality}{15}{subsection.2.3.3}}
\newlabel{sec:optimal_value}{{2.3.3}{15}{Value Function and Optimality}{subsection.2.3.3}{}}
\citation{sutton2011reinforcement}
\citation{model_free_prediction}
\citation{sutton2011reinforcement}
\citation{mdp_dynamic_prog}
\citation{sutton2011reinforcement}
\newlabel{eq:value_function}{{2.16}{16}{Value Function and Optimality}{equation.2.3.16}{}}
\newlabel{eq:optimal_value}{{2.17}{16}{Value Function and Optimality}{equation.2.3.17}{}}
\newlabel{eq:bellman_optimal}{{2.18}{16}{Value Function and Optimality}{equation.2.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Temporal Difference Learning}{16}{subsection.2.3.4}}
\newlabel{sec:td_learning}{{2.3.4}{16}{Temporal Difference Learning}{subsection.2.3.4}{}}
\citation{sutton2011reinforcement}
\citation{exploration_vs_exploitation}
\newlabel{eq:sarsa}{{2.19}{17}{Sarsa}{equation.2.3.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Q-Learning}{17}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{17}{section*.24}}
\newlabel{eq:expected_sarsa}{{2.22}{17}{Expected Sarsa}{equation.2.3.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Exploration vs Exploitation}{17}{subsection.2.3.5}}
\newlabel{sec:exploration_vs_exploitation}{{2.3.5}{17}{Exploration vs Exploitation}{subsection.2.3.5}{}}
\citation{sutton2011reinforcement}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Linking TD-Learning and Light Transport Simulation}{18}{section.2.4}}
\newlabel{sec:td_light_transport}{{2.4}{18}{Linking TD-Learning and Light Transport Simulation}{section.2.4}{}}
\newlabel{eq:sarsa_integral}{{2.25}{18}{Linking TD-Learning and Light Transport Simulation}{equation.2.4.25}{}}
\citation{greger1998irradiance}
\newlabel{eq:expected_sarsa_td_learning}{{2.26}{19}{Linking TD-Learning and Light Transport Simulation}{equation.2.4.26}{}}
\newlabel{eq:mc_expected_sarsa_td_learning}{{2.27}{19}{Linking TD-Learning and Light Transport Simulation}{equation.2.4.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces An illustration of a simple incident radiance distribution for a given point $x$ in a scene. Where $\omega _i$ $\forall \omega _i \in \Omega $ are all possible incident directions in the hemisphere around the point $x$ oriented to the surface normal at $x$.The scene contains two area lights with the same power which are visible from $x$ in the scene, causing two equal level peaks in the incident radiance distribution for $x$.\relax }}{19}{figure.caption.25}}
\newlabel{fig:incident_radiance_distributio }{{2.9}{19}{An illustration of a simple incident radiance distribution for a given point $x$ in a scene. Where $\omega _i$ $\forall \omega _i \in \Omega $ are all possible incident directions in the hemisphere around the point $x$ oriented to the surface normal at $x$.The scene contains two area lights with the same power which are visible from $x$ in the scene, causing two equal level peaks in the incident radiance distribution for $x$.\relax }{figure.caption.25}{}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\citation{greger1998irradiance}
\citation{shirley1994notes}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}TD-Learning and Deep Reinforcement Learning for Importance Sampling Light Paths}{20}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:td_deep_sampling}{{3}{20}{TD-Learning and Deep Reinforcement Learning for Importance Sampling Light Paths}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The expected Sarsa Path Tracer}{20}{section.3.1}}
\newlabel{sec:expecte_sarsa_path_tracer}{{3.1}{20}{The expected Sarsa Path Tracer}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}The Irradiance Volume}{20}{subsection.3.1.1}}
\citation{bentley1975multidimensional}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An Irradiance Volume. Each sector holds the incoming radiance $L_i(x,\omega _k)$, the more green a sector is the lower the stored radiance in that sector, the more red a sector is the higher the stored radiance in that sector. \relax }}{21}{figure.caption.27}}
\newlabel{fig:irradiance_volume}{{3.1}{21}{An Irradiance Volume. Each sector holds the incoming radiance $L_i(x,\omega _k)$, the more green a sector is the lower the stored radiance in that sector, the more red a sector is the higher the stored radiance in that sector. \relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example of discretizing location in the scene into Irradiance Volume locations. The geometry mesh (a) is used to uniformly sample Irradiance volume positions. Image (b) shows a voronoi plot for the Irradiance Volumes in the scene, where each pixel is coloured to the represent its closest Irradiance Volume, so each sector of colour in (b) represents a different Irradiance Volume location. Finally (c) gives a render using the Expected Sarsa path tracer based on Algorithm \ref  {alg:expected_sarsa_pathtracer}.\relax }}{21}{figure.caption.28}}
\newlabel{fig:scene_discretization_example}{{3.2}{21}{An example of discretizing location in the scene into Irradiance Volume locations. The geometry mesh (a) is used to uniformly sample Irradiance volume positions. Image (b) shows a voronoi plot for the Irradiance Volumes in the scene, where each pixel is coloured to the represent its closest Irradiance Volume, so each sector of colour in (b) represents a different Irradiance Volume location. Finally (c) gives a render using the Expected Sarsa path tracer based on Algorithm \ref {alg:expected_sarsa_pathtracer}.\relax }{figure.caption.28}{}}
\citation{devroye2006nonuniform}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Expected Sarsa Path Tracing}{22}{subsection.3.1.2}}
\newlabel{sec:expected_sarsa_path_tracer}{{3.1.2}{22}{Expected Sarsa Path Tracing}{subsection.3.1.2}{}}
\newlabel{alg:expected_sarsa_pathtracer}{{2}{23}{Addition 3: Update Irradiance Volume Distributions}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Expected Sarsa path tracer pseudo code following Nvidia's method in \cite  {dahm2017learning}. Given a camera position, scene geometry, this algorithm will render a single image using a tabular Expected Sarsa approach to progressively reduce image noise. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{23}{algocf.2}}
\newlabel{eq:mc_expected_sarsa_pdf}{{3.1}{23}{Monte Carlo Integration}{equation.3.1.1}{}}
\citation{dahm2017learning}
\newlabel{fig:uniform_pdf}{{3.3a}{24}{Hemisphere with a uniform $pdf$\relax }{figure.caption.33}{}}
\newlabel{sub@fig:uniform_pdf}{{a}{24}{Hemisphere with a uniform $pdf$\relax }{figure.caption.33}{}}
\newlabel{fig:not_uniform_pdf}{{3.3b}{24}{Irradiance Volume with a non-uniform $pdf$\relax }{figure.caption.33}{}}
\newlabel{sub@fig:not_uniform_pdf}{{b}{24}{Irradiance Volume with a non-uniform $pdf$\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A 2 dimension view of a subset of values from two probability density functions ($pdf$). One for a unit hemisphere (left) with a uniform $pdf$. One for an Irradiance Volume (right) with non-uniform pdf. Where the arrows represent sampled directions and the values at the end are the evaluated $pdf$ values for each direction.\relax }}{24}{figure.caption.33}}
\newlabel{fig:pdfs}{{3.3}{24}{A 2 dimension view of a subset of values from two probability density functions ($pdf$). One for a unit hemisphere (left) with a uniform $pdf$. One for an Irradiance Volume (right) with non-uniform pdf. Where the arrows represent sampled directions and the values at the end are the evaluated $pdf$ values for each direction.\relax }{figure.caption.33}{}}
\newlabel{eq:decay_lr}{{3.2}{24}{Consistency}{equation.3.1.2}{}}
\citation{deep_rl_function_approx}
\citation{sutton2011reinforcement}
\citation{sutton1996generalization}
\citation{konidaris2011value}
\citation{uther1998tree}
\citation{lecun2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Neural-Q Path Tracer}{25}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to Deep Reinforcement Learning}{25}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Value Function Approximation}{25}{section*.35}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{25}{section*.36}}
\newlabel{eq:example_loss}{{3.3}{25}{Stochastic Gradient Descent}{equation.3.2.3}{}}
\citation{deep_rl_function_approx}
\citation{lillicrap2015continuous}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{Bootstrapping}{26}{section*.37}}
\newlabel{sec:bootstrapping}{{3.2.1}{26}{Bootstrapping}{section*.37}{}}
\newlabel{eq:td_error_q_learning}{{3.4}{26}{Bootstrapping}{equation.3.2.4}{}}
\newlabel{eq:loss_q_learning}{{3.5}{26}{Bootstrapping}{equation.3.2.5}{}}
\newlabel{eq:q_learning_derv_loss}{{3.6}{26}{Bootstrapping}{equation.3.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Deep Reinforcement Learning Motivation}{26}{subsection.3.2.2}}
\newlabel{sec:deep_rl_motivation}{{3.2.2}{26}{Deep Reinforcement Learning Motivation}{subsection.3.2.2}{}}
\citation{mnih2013playing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Deep Q-learning for Light Transport}{27}{subsection.3.2.3}}
\newlabel{eq:neural_q_loss}{{3.7}{27}{Deep Q-learning for Light Transport}{equation.3.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Artificial Neural Network Architecture}{27}{subsection.3.2.4}}
\newlabel{sec:ann_architecture}{{3.2.4}{27}{Artificial Neural Network Architecture}{subsection.3.2.4}{}}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\citation{mnih2013playing}
\citation{nair2010rectified}
\citation{ren2013global}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces An illustration of the process for a forward pass on the ANN used for the Neural-Q path tracer. Starting with the scene on the left, all vertices in the scene are converted into a coordinate system relative to the point $x$, producing $\mathbf  {v^x}$. This is passed to the input layer of the ANN which then computes a forward pass through the two hidden layers fully connected layers, each with a width of $200$ neurons to the output layer. The output layers width is equal to the number of discrete incident directions $m$ used in the approximation of the incident radiance function. Each output represents the approximated incident radiance on the position $x$ for every discrete incident direction $\mathaccentV {hat}05E{q_\theta }(x, \omega _k)$ $\forall k = 1, ..., m$.\relax }}{28}{figure.caption.38}}
\newlabel{fig:nn_radiance_estimate_illustration}{{3.4}{28}{An illustration of the process for a forward pass on the ANN used for the Neural-Q path tracer. Starting with the scene on the left, all vertices in the scene are converted into a coordinate system relative to the point $x$, producing $\mathbf {v^x}$. This is passed to the input layer of the ANN which then computes a forward pass through the two hidden layers fully connected layers, each with a width of $200$ neurons to the output layer. The output layers width is equal to the number of discrete incident directions $m$ used in the approximation of the incident radiance function. Each output represents the approximated incident radiance on the position $x$ for every discrete incident direction $\hat {q_\theta }(x, \omega _k)$ $\forall k = 1, ..., m$.\relax }{figure.caption.38}{}}
\citation{sutton2011reinforcement}
\citation{deep_rl_function_approx}
\citation{lillicrap2015continuous}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Neural-Q Path Tracing Algorithm}{29}{subsection.3.2.5}}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\citation{muller2018neural}
\newlabel{alg:neural_q_pathtracer}{{3}{31}{Addition 4: Decaying decaying $\epsilon $-\textit {greedy}}{algocfline.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Neural-Q forward path tracer. Given a camera position, scene geometry, epsilon and epsilon decay, this algorithm will render a single image using deep Q-learning loss to progressively reduce image noise. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{31}{algocf.3}}
\citation{glm}
\citation{sdl2}
\citation{dynet}
\citation{cuda}
\citation{tensorflow2015-whitepaper}
\citation{maya}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Critical Evaluation}{32}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{4}{32}{Critical Evaluation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{32}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Assessing the reduction in image Noise for Monte Carlo Path Tracing}{32}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Quantifying the Reduction in Image Noise}{32}{subsection.4.2.1}}
\citation{christensen2016path}
\newlabel{eq:mape}{{4.1}{33}{Quantifying the Reduction in Image Noise}{equation.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A comparison of the default forward path tracer, Expected Sarsa path tracer, and the Neural-Q path tracer of their image noise for four different rendered scenes. All scenes were rendered with 128 samples per pixel. The score under each column in an image row corresponds to the MAPE score for each path tracing algorithm for the particular scene.The Neural-Q and Expected Sarsa algorithms both used $144$ equally spaced directions to estimate the radiance distribution on a given point. The Neural-Q path tracer used the network described in \ref  {sec:ann_architecture} for all fours scenes with a decaying $\epsilon $-\textit  {greedy} policy start at $\epsilon =1$ with a decay of $\delta = 0.05$ applied after every pixel in the image has had a light path sampled through it once. The Expected Sarsa path tracer used just enough Irradiance Volumes (which varied depending on the scene) to facilitate a significant reduction in image noise in all four renders.\relax }}{34}{figure.caption.43}}
\newlabel{fig:mape_results_grid}{{4.1}{34}{A comparison of the default forward path tracer, Expected Sarsa path tracer, and the Neural-Q path tracer of their image noise for four different rendered scenes. All scenes were rendered with 128 samples per pixel. The score under each column in an image row corresponds to the MAPE score for each path tracing algorithm for the particular scene.The Neural-Q and Expected Sarsa algorithms both used $144$ equally spaced directions to estimate the radiance distribution on a given point. The Neural-Q path tracer used the network described in \ref {sec:ann_architecture} for all fours scenes with a decaying $\epsilon $-\textit {greedy} policy start at $\epsilon =1$ with a decay of $\delta = 0.05$ applied after every pixel in the image has had a light path sampled through it once. The Expected Sarsa path tracer used just enough Irradiance Volumes (which varied depending on the scene) to facilitate a significant reduction in image noise in all four renders.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}A Closer Inspection of Pixel Error Values}{35}{subsection.4.2.2}}
\newlabel{sec:close_inspec_pixels}{{4.2.2}{35}{A Closer Inspection of Pixel Error Values}{subsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Histograms for the average RGB pixel error values for all four rendered scenes using both the Expected Sarsa path tracer and the Neural-Q path tracer. Where the average RGB pixel error value is the average difference in all RGB colour channels between a pixel in the reference image, and the corresponding pixel in the image rendered by either the Expected Sarsa or Neural-Q path tracers. The max average RGB pixel error value is $255$, which corresponds to the case where the reference images pixels value was $(255,255,255)$ whereas the rendered images was $(0,0,0)$ or vice versa. The histograms are based on the rendered images presented in Figure \ref  {fig:mape_results_grid}.\relax }}{35}{figure.caption.44}}
\newlabel{fig:histogram_errors}{{4.2}{35}{Histograms for the average RGB pixel error values for all four rendered scenes using both the Expected Sarsa path tracer and the Neural-Q path tracer. Where the average RGB pixel error value is the average difference in all RGB colour channels between a pixel in the reference image, and the corresponding pixel in the image rendered by either the Expected Sarsa or Neural-Q path tracers. The max average RGB pixel error value is $255$, which corresponds to the case where the reference images pixels value was $(255,255,255)$ whereas the rendered images was $(0,0,0)$ or vice versa. The histograms are based on the rendered images presented in Figure \ref {fig:mape_results_grid}.\relax }{figure.caption.44}{}}
\citation{dahm2017learning}
\newlabel{fig:true_incident_distribution}{{4.3a}{36}{Ground Truth\relax }{figure.caption.45}{}}
\newlabel{sub@fig:true_incident_distribution}{{a}{36}{Ground Truth\relax }{figure.caption.45}{}}
\newlabel{fig:exp_sarsa_distribution}{{4.3b}{36}{Expected Sarsa\relax }{figure.caption.45}{}}
\newlabel{sub@fig:exp_sarsa_distribution}{{b}{36}{Expected Sarsa\relax }{figure.caption.45}{}}
\newlabel{fig:neural_q_distribution}{{4.3c}{36}{Neural-Q\relax }{figure.caption.45}{}}
\newlabel{sub@fig:neural_q_distribution}{{c}{36}{Neural-Q\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualisations of the incident radiance distribution for three different positions in a scene. The incident radiance is represented by placing an adaptive quadrature at each point with 144 different sectors, each representing an incident direction which the incident radiance has been calculated at. The positions are in front of a single large area light within a simple scene. (a) presents the ground truth incident radiance distribution which has been calculated by Monte Carlo integration by firing 4096 sampled light paths per sector. (b) is the approximated incident radiance distribution for the three positions produced by the Expected Sarsa method when trained 1024 SPP, and (c) is the same but for the Neural-Q method trained with 1024 SPP. Due to the single large area light in front of the point, most of the incident radiance on the points will come from directions spanning from the area light on the left.\relax }}{36}{figure.caption.45}}
\newlabel{fig:distribution_visualisation}{{4.3}{36}{Visualisations of the incident radiance distribution for three different positions in a scene. The incident radiance is represented by placing an adaptive quadrature at each point with 144 different sectors, each representing an incident direction which the incident radiance has been calculated at. The positions are in front of a single large area light within a simple scene. (a) presents the ground truth incident radiance distribution which has been calculated by Monte Carlo integration by firing 4096 sampled light paths per sector. (b) is the approximated incident radiance distribution for the three positions produced by the Expected Sarsa method when trained 1024 SPP, and (c) is the same but for the Neural-Q method trained with 1024 SPP. Due to the single large area light in front of the point, most of the incident radiance on the points will come from directions spanning from the area light on the left.\relax }{figure.caption.45}{}}
\newlabel{fig:dist_expected_sarsa}{{4.4a}{36}{Expected Sarsa\relax }{figure.caption.46}{}}
\newlabel{sub@fig:dist_expected_sarsa}{{a}{36}{Expected Sarsa\relax }{figure.caption.46}{}}
\newlabel{fig:dist_neural_q}{{4.4b}{36}{Neural-Q\relax }{figure.caption.46}{}}
\newlabel{sub@fig:dist_neural_q}{{b}{36}{Neural-Q\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An illustration of the incident radiance distribution for a given point directly in front of the light source shown in Figure \ref  {fig:distribution_visualisation} for both the Expected Sarsa and Neural-Q methods. Where $\omega _i \in \Omega $ is discrete incident angles on the point, and $p(\omega _i)$ is the probability density function over incident radiance evaluated at angle $\omega _i$. The \textbf  {red} line represents the true incident radiance distribution on the point, the \textbf  {blue} represents the corresponding methods approximation of the incident radiance distribution on the point.\relax }}{36}{figure.caption.46}}
\newlabel{fig:dist_2_methods}{{4.4}{36}{An illustration of the incident radiance distribution for a given point directly in front of the light source shown in Figure \ref {fig:distribution_visualisation} for both the Expected Sarsa and Neural-Q methods. Where $\omega _i \in \Omega $ is discrete incident angles on the point, and $p(\omega _i)$ is the probability density function over incident radiance evaluated at angle $\omega _i$. The \textbf {red} line represents the true incident radiance distribution on the point, the \textbf {blue} represents the corresponding methods approximation of the incident radiance distribution on the point.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}From a Discrete to Continuous State Space}{37}{section.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Reference Images of the The Door Room and Shelter scenes next to SPP renderusing both na trained Expected Sarsa path tracer, and a trained Neural-Q path tracer trained for 100 epochs. The single sample for each pixel in the Expected Sarsa and Neural-Q renders is constructed by reflecting the light path in the direction of highest estimated radiance upon every intersection point, until the light is intersected with.\relax }}{37}{figure.caption.47}}
\newlabel{fig:1_spp_max_dir}{{4.5}{37}{Reference Images of the The Door Room and Shelter scenes next to SPP renderusing both na trained Expected Sarsa path tracer, and a trained Neural-Q path tracer trained for 100 epochs. The single sample for each pixel in the Expected Sarsa and Neural-Q renders is constructed by reflecting the light path in the direction of highest estimated radiance upon every intersection point, until the light is intersected with.\relax }{figure.caption.47}{}}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Convergence}{38}{section.4.4}}
\newlabel{sec:convergence_learning_incident}{{4.4}{38}{Convergence}{section.4.4}{}}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{hyperion}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Training curves for the average path length and number of zero contribution light paths when rendering the Shelter scene for 100 epochs, as well as the 1st and 100th rendered frame when using 1 SPP for both images. This is shown for both the Expected Sarsa and Neural-Q path tracers. An epoch represents one sampled light ray through every pixel in the image. The average path length is the number reflections a light path takes before intersecting with a light source. A zero contribution light path is one which contributes almost zero colour to the final image.\relax }}{39}{figure.caption.48}}
\newlabel{fig:training_curves_archway}{{4.6}{39}{Training curves for the average path length and number of zero contribution light paths when rendering the Shelter scene for 100 epochs, as well as the 1st and 100th rendered frame when using 1 SPP for both images. This is shown for both the Expected Sarsa and Neural-Q path tracers. An epoch represents one sampled light ray through every pixel in the image. The average path length is the number reflections a light path takes before intersecting with a light source. A zero contribution light path is one which contributes almost zero colour to the final image.\relax }{figure.caption.48}{}}
\citation{dahm2017learning}
\citation{devastator}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}From Memory Bound to Compute Bound}{40}{section.4.5}}
\newlabel{sec:mem_to_comp}{{4.5}{40}{From Memory Bound to Compute Bound}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Memory Usage}{40}{subsection.4.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Additional memory required in Megabytes (MB) to render each of the four scenes shown in Figure \ref  {fig:mape_results_grid} using both the Expected Sarsa and Neural-Q path tracers.\relax }}{40}{table.caption.49}}
\newlabel{tab:memory_usage}{{4.1}{40}{Additional memory required in Megabytes (MB) to render each of the four scenes shown in Figure \ref {fig:mape_results_grid} using both the Expected Sarsa and Neural-Q path tracers.\relax }{table.caption.49}{}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{40}{section*.50}}
\citation{ren2013global}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The Complex Pillars scene rendered using two Expected Sarsa path tracers both trained for 1000 SPP and the rendered images in the middle column are produced using 128 SPP. The first row presents the Expected Sarsa path tracer with 36029 Irradiance Volumes. The second row is the Expected Sarsa path tracer with only 780 Irradiance Volumes. The left column is a voronoi plot of the Irradiance Volumes, where each pixel is given a colour to represent which Irradiance Volume it is closest to. The MAPE score and render is given for both renders in the middle column. The right column presents two close-ups to give a visual comparison of the noise between the two images.\relax }}{41}{figure.caption.51}}
\newlabel{fig:voronoi_difference}{{4.7}{41}{The Complex Pillars scene rendered using two Expected Sarsa path tracers both trained for 1000 SPP and the rendered images in the middle column are produced using 128 SPP. The first row presents the Expected Sarsa path tracer with 36029 Irradiance Volumes. The second row is the Expected Sarsa path tracer with only 780 Irradiance Volumes. The left column is a voronoi plot of the Irradiance Volumes, where each pixel is given a colour to represent which Irradiance Volume it is closest to. The MAPE score and render is given for both renders in the middle column. The right column presents two close-ups to give a visual comparison of the noise between the two images.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q}{41}{section*.53}}
\citation{crockett1995parallel}
\citation{alerstam2008parallel}
\citation{fatahalian2009data}
\citation{embarissingly_parallelizable}
\citation{accelerated_ray_tracing}
\citation{hyperion}
\citation{raynal2012concurrent}
\citation{cuda}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Cornell Box scene rendered using 1MB of memory to store the Irradiance Volumes for the Expected Sarsa path tracer. The large black artefacts and incorrect colouring of the red block is a result of using too few Irradiance Volumes to approximate the incident radiance function $L_i(x, \omega )$. Causing importance sampling to actually increase the variance in pixel value estimates, leading to an increase in overall image noise.\relax }}{42}{figure.caption.52}}
\newlabel{fig:incorrect_cornell}{{4.8}{42}{Cornell Box scene rendered using 1MB of memory to store the Irradiance Volumes for the Expected Sarsa path tracer. The large black artefacts and incorrect colouring of the red block is a result of using too few Irradiance Volumes to approximate the incident radiance function $L_i(x, \omega )$. Causing importance sampling to actually increase the variance in pixel value estimates, leading to an increase in overall image noise.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Parallel Processing}{42}{subsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{42}{section*.54}}
\citation{cuda_c_guide}
\citation{global_vs_shared}
\citation{dahm2017learning}
\citation{muller2018neural}
\citation{alg:neural_q_path_tracer}
\citation{global_data_transfer}
\citation{muller2018neural}
\citation{keller2019integral}
\citation{tensor_cores}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q}{43}{section*.55}}
\citation{georgiev2018arnold}
\citation{georgiev2018arnold}
\citation{dahm2017learning}
\citation{mnih2013playing}
\citation{van2016deep}
\citation{mnih2015human}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{hyperion}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Hyperparameters}{44}{section.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{44}{section*.56}}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q}{44}{section*.57}}
\citation{dahm2017learning}
\citation{muller2017practical}
\citation{vorba2014line}
\citation{zheng2018learning}
\citation{muller2018neural}
\citation{keller2019integral}
\citation{hermosilla2018deep}
\citation{muller2017practical}
\citation{zheng2018learning}
\citation{dahm2017learning}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{dinh2014nice}
\citation{muller2018neural}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Recent Advancements in Neural Importance Sampling for Monte Carlo Path Tracing}{45}{section.4.7}}
\newlabel{sec:recent_advancements}{{4.7}{45}{Recent Advancements in Neural Importance Sampling for Monte Carlo Path Tracing}{section.4.7}{}}
\citation{keller2015path}
\citation{akenine2018real}
\citation{vorba2014line}
\citation{muller2017practical}
\citation{dahm2017learning}
\citation{dahm2017learning}
\citation{mnih2013playing}
\citation{kajiya1986rendering}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{46}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{46}{Conclusions}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Wider Motivation and the Problem}{46}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Summary of Contributions}{46}{section.5.2}}
\citation{keller2019integral}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Discussion}{47}{section.5.3}}
\citation{keller2019integral}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{zheng2018learning}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{muller2018neural}
\citation{nvidia_turing_architecture_whitepaper_2018}
\citation{tensor_cores}
\citation{boulos2005notes}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{bako2017kernel}
\citation{chaitanya2017interactive}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Future Work}{48}{section.5.4}}
\@writefile{toc}{\contentsline {subsubsection}{Scaling up Neural-Q}{48}{section*.62}}
\citation{bishop2006pattern}
\citation{muller2018neural}
\citation{van2016deep}
\citation{sutton2011reinforcement}
\citation{keller2019integral}
\citation{lillicrap2015continuous}
\citation{mnih2016asynchronous}
\@writefile{toc}{\contentsline {subsubsection}{Investigating the relationship between scene geometry and ANN requirements}{49}{section*.63}}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q versus Disney's Neural Path Guider}{49}{section*.64}}
\@writefile{toc}{\contentsline {subsubsection}{Further investigate Deep Reinforcement learning techniques}{49}{section*.65}}
\bibdata{dissertation}
\bibcite{glm}{1}
\bibcite{sdl2}{2}
\bibcite{tensorflow2015-whitepaper}{3}
\bibcite{akenine2018real}{4}
\bibcite{alerstam2008parallel}{5}
\bibcite{accelerated_ray_tracing}{6}
\bibcite{maya}{7}
\bibcite{bako2017kernel}{8}
\bibcite{bashford2012significance}{9}
\bibcite{bentley1975multidimensional}{10}
\bibcite{bishop2006pattern}{11}
\bibcite{bloomberg.com}{12}
\bibcite{boulos2005notes}{13}
\bibcite{chaitanya2017interactive}{14}
\bibcite{christensen2018renderman}{15}
\bibcite{christensen2016path}{16}
\bibcite{cline2008table}{17}
\bibcite{crockett1995parallel}{18}
\bibcite{dahm2017learning}{19}
\bibcite{devroye2006nonuniform}{20}
\bibcite{dinh2014nice}{21}
\bibcite{dutre2004state}{22}
\bibcite{fatahalian2009data}{23}
\bibcite{embarissingly_parallelizable}{24}
\bibcite{georgiev2018arnold}{25}
\bibcite{glassner2014principles}{26}
\bibcite{greger1998irradiance}{27}
\bibcite{stanford_graphics}{28}
\bibcite{global_data_transfer}{29}
\bibcite{global_vs_shared}{30}
\bibcite{hermosilla2018deep}{31}
\bibcite{jensen1996global}{32}
\bibcite{kajiya1986rendering}{33}
\bibcite{keller2019integral}{34}
\bibcite{keller2016path}{35}
\bibcite{keller2015path}{36}
\bibcite{konidaris2011value}{37}
\bibcite{krivanek2014recent}{38}
\bibcite{lecun2015deep}{39}
\bibcite{lillicrap2015continuous}{40}
\bibcite{mnih2016asynchronous}{41}
\bibcite{mnih2013playing}{42}
\bibcite{mnih2015human}{43}
\bibcite{morokoff1995quasi}{44}
\bibcite{muller2017practical}{45}
\bibcite{muller2018neural}{46}
\bibcite{nair2010rectified}{47}
\bibcite{dynet}{48}
\bibcite{cuda}{49}
\bibcite{tensor_cores}{50}
\bibcite{cuda_c_guide}{51}
\bibcite{nvidia_turing_architecture_whitepaper_2018}{52}
\bibcite{pan2006virtual}{53}
\bibcite{pegoraro2008towards}{54}
\bibcite{ramamoorthi2012theory}{55}
\bibcite{raynal2012concurrent}{56}
\bibcite{ren2013global}{57}
\bibcite{devastator}{58}
\bibcite{scratchapixel_2015}{59}
\bibcite{shirley1994notes}{60}
\bibcite{hyperion}{61}
\bibcite{sutton1996generalization}{62}
\bibcite{sutton2011reinforcement}{63}
\bibcite{uther1998tree}{64}
\bibcite{exploration_vs_exploitation}{65}
\bibcite{deep_rl_function_approx}{66}
\bibcite{introToRL}{67}
\bibcite{mdp_dynamic_prog}{68}
\bibcite{model_free_prediction}{69}
\bibcite{van2016deep}{70}
\bibcite{vorba2014line}{71}
\bibcite{normals}{72}
\bibcite{zheng2018learning}{73}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}An Example Appendix}{54}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appx:example}{{A}{54}{An Example Appendix}{appendix.A}{}}
