\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\citation{latexbook1}
\citation{latexbook2}
\providecommand \oddpage@label [2]{}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.0.1}Plan}{xix}{subsection.0.0.1}}
\citation{christensen2016path}
\citation{keller2016path}
\citation{krivanek2014recent}
\citation{jensen1996global}
\citation{keller2016path}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Contextual Background}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:context}{{1}{1}{Contextual Background}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Path Tracing for Light Transport Simulation}{1}{section.1.1}}
\newlabel{sec:conceptual_path_trace}{{1.1}{1}{Path Tracing for Light Transport Simulation}{section.1.1}{}}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Temporal Difference Learning for Importance Sampling Ray Directions}{2}{section.1.2}}
\newlabel{sec:td_learn_for_importance}{{1.2}{2}{Temporal Difference Learning for Importance Sampling Ray Directions}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is Temporal Difference learning?}{2}{subsection.1.2.1}}
\citation{ramamoorthi2012theory}
\citation{dahm2017learning}
\citation{pan2006virtual}
\citation{bloomberg.com}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Temporal Difference learning methods for Efficient Light Transport Simulation}{3}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Why use Temporal Difference Learning for Importance Sampling?}{3}{subsection.1.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Motivation}{3}{section.1.3}}
\newlabel{sec:motivation}{{1.3}{3}{Motivation}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Real time Rendering using Accurate Light Transport Simulation}{3}{subsection.1.3.1}}
\citation{nvidia_turing_architecture_whitepaper_2018}
\citation{bako2017kernel}
\citation{chaitanya2017interactive}
\citation{christensen2018renderman}
\citation{georgiev2018arnold}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Recent Developments}{4}{subsection.1.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Challenges and Objectives}{4}{section.1.4}}
\citation{morokoff1995quasi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Technical Background}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:technical}{{2}{7}{Technical Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Monte Carlo Integration and Importance Sampling}{7}{section.2.1}}
\newlabel{sec:monte_carlo}{{2.1}{7}{Monte Carlo Integration and Importance Sampling}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Monte Carlo Integration}{7}{subsection.2.1.1}}
\newlabel{sec:monte_carlo_approx}{{2.1.1}{7}{Monte Carlo Integration}{subsection.2.1.1}{}}
\newlabel{eq:integral}{{2.1}{7}{Monte Carlo Integration}{equation.2.1.1}{}}
\newlabel{eq:monte_carlo}{{2.2}{7}{Monte Carlo Integration}{equation.2.1.2}{}}
\newlabel{eq:generalized_mc}{{2.3}{7}{Monte Carlo Integration}{equation.2.1.3}{}}
\newlabel{eq:unbiased}{{2.4}{7}{Monte Carlo Integration}{equation.2.1.4}{}}
\citation{morokoff1995quasi}
\citation{scratchapixel_2015}
\newlabel{eq:law_large_numbers}{{2.5}{8}{Monte Carlo Integration}{equation.2.1.5}{}}
\newlabel{eq:sample_variance}{{2.6}{8}{Monte Carlo Integration}{equation.2.1.6}{}}
\newlabel{eq:mc_error}{{2.7}{8}{Monte Carlo Integration}{equation.2.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Importance Sampling for Reducing Approximation Variance}{8}{subsection.2.1.2}}
\newlabel{sec:importance_smapling}{{2.1.2}{8}{Importance Sampling for Reducing Approximation Variance}{subsection.2.1.2}{}}
\newlabel{eq:constant_monte_carlo}{{2.8}{8}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Constant Function  with a sample point\relax }}{8}{figure.caption.12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:constant_function}{{2.1}{8}{Constant Function\\ with a sample point\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Non-linear Function  with a sample point\relax }}{8}{figure.caption.12}}
\newlabel{fig:non_lin_function}{{2.2}{8}{Non-linear Function\\ with a sample point\relax }{figure.caption.12}{}}
\citation{kajiya1986rendering}
\newlabel{eq:constant_conversion}{{2.9}{9}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.9}{}}
\newlabel{eq:solve_mc_integration}{{2.10}{9}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.10}{}}
\newlabel{fig:improtance_correct}{{2.3a}{9}{Importance sampling reducing variance\relax }{figure.caption.13}{}}
\newlabel{sub@fig:improtance_correct}{{a}{9}{Importance sampling reducing variance\relax }{figure.caption.13}{}}
\newlabel{fig:improtance_uniform}{{2.3b}{9}{Uniform\\ sampling\relax }{figure.caption.13}{}}
\newlabel{sub@fig:improtance_uniform}{{b}{9}{Uniform\\ sampling\relax }{figure.caption.13}{}}
\newlabel{fig:improtance_incorrect}{{2.3c}{9}{Importance sampling increasing variance\relax }{figure.caption.13}{}}
\newlabel{sub@fig:improtance_incorrect}{{c}{9}{Importance sampling increasing variance\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Graphical representation of a function $f(x)$ (red) and the corresponding probability density function $pdf(x)$ (blue) used in the Monte Carlo integration approximation for the integral of $f(x)$.\relax }}{9}{figure.caption.13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Monte Carlo Path Tracing}{9}{section.2.2}}
\newlabel{sec:mc_pathtracing}{{2.2}{9}{Monte Carlo Path Tracing}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Rendering Equation}{9}{subsection.2.2.1}}
\newlabel{sec:rendering_equation}{{2.2.1}{9}{The Rendering Equation}{subsection.2.2.1}{}}
\citation{dutre2004state}
\citation{glassner2014principles}
\newlabel{eq:rendering_equation}{{2.11}{10}{The Rendering Equation}{equation.2.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(h(x, \omega _i),-\omega _i) = L_i(y_i, -\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(y_i, -\omega _i)$ is identical to calculating the outgoing radiance $L_o(y_i, -\omega _i)$ as we assume no radiance is lost along a ray line, hence the $L_o$ is a recursive function. \relax }}{10}{figure.caption.14}}
\newlabel{fig:recursive_rendering}{{2.4}{10}{A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(h(x, \omega _i),-\omega _i) = L_i(y_i, -\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(y_i, -\omega _i)$ is identical to calculating the outgoing radiance $L_o(y_i, -\omega _i)$ as we assume no radiance is lost along a ray line, hence the $L_o$ is a recursive function. \relax }{figure.caption.14}{}}
\citation{normals}
\citation{stanford_graphics}
\newlabel{fig:diffuse_brdf}{{2.5a}{11}{Diffuse BRDF\relax }{figure.caption.15}{}}
\newlabel{sub@fig:diffuse_brdf}{{a}{11}{Diffuse BRDF\relax }{figure.caption.15}{}}
\newlabel{fig:specular_brdf}{{2.5b}{11}{Specular BRDF\relax }{figure.caption.15}{}}
\newlabel{sub@fig:specular_brdf}{{b}{11}{Specular BRDF\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A representation of both a diffuse surface and specular surface BRDF for a given angle of incidence $\omega '$. The surface point is located where all end of the arrows converge. The arrows indicate a subset of direction possible for the incident ray to be reflected in. All possible directions reflected directions for a ray are defined between the surface point and the line , for an incident direction $\omega '$. The further away a point is on the line, the more likely a ray is to reflected in a direction from the surface point to that point on the line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favour a small subset are of direction in the hemisphere surrounding the surface point.\relax }}{11}{figure.caption.15}}
\newlabel{fig:brdfs}{{2.5}{11}{A representation of both a diffuse surface and specular surface BRDF for a given angle of incidence $\omega '$. The surface point is located where all end of the arrows converge. The arrows indicate a subset of direction possible for the incident ray to be reflected in. All possible directions reflected directions for a ray are defined between the surface point and the line , for an incident direction $\omega '$. The further away a point is on the line, the more likely a ray is to reflected in a direction from the surface point to that point on the line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favour a small subset are of direction in the hemisphere surrounding the surface point.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two sculptures, one made from a diffuse material (left) and the other from a specular material.\relax }}{11}{figure.caption.16}}
\newlabel{fig:material_pics}{{2.6}{11}{Two sculptures, one made from a diffuse material (left) and the other from a specular material.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Path Tracing}{11}{subsection.2.2.2}}
\citation{christensen2016path}
\newlabel{eq:rendering_eq_monte_carlo}{{2.12}{12}{Path Tracing}{equation.2.2.12}{}}
\newlabel{alg:forward_path_tracing}{{1}{12}{Path Tracing}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Forward path tracer\relax }}{12}{algocf.1}}
\citation{bashford2012significance}
\citation{cline2008table}
\citation{pegoraro2008towards}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning and TD-Learning}{13}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Markov Decision Processes}{13}{subsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Markov Decision Process \cite  {sutton2011reinforcement}\relax }}{13}{figure.caption.17}}
\newlabel{fig:mdp}{{2.7}{13}{Markov Decision Process \cite {sutton2011reinforcement}\relax }{figure.caption.17}{}}
\citation{sutton2011reinforcement}
\newlabel{eq:markov_property}{{2.13}{14}{Markov Decision Processes}{equation.2.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Goals and Rewards}{14}{subsection.2.3.2}}
\newlabel{eq:return}{{2.14}{14}{Goals and Rewards}{equation.2.3.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Linking TD-Learning and Light Transport Simulation}{14}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}The expected SARSA Path tracer}{14}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Plan}{14}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Q-Learning Path tracer}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:deep-q}{{3}{17}{Deep Q-Learning Path tracer}{chapter.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.1}Plan}{17}{subsection.3.0.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Critical Evaluation}{19}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{4}{19}{Critical Evaluation}{chapter.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Plan}{19}{subsection.4.0.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{21}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{21}{Conclusion}{chapter.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Plan}{21}{subsection.5.0.1}}
\bibdata{dissertation}
\bibcite{bako2017kernel}{1}
\bibcite{bashford2012significance}{2}
\bibcite{bloomberg.com}{3}
\bibcite{chaitanya2017interactive}{4}
\bibcite{christensen2018renderman}{5}
\bibcite{christensen2016path}{6}
\bibcite{cline2008table}{7}
\bibcite{dahm2017learning}{8}
\bibcite{dutre2004state}{9}
\bibcite{georgiev2018arnold}{10}
\bibcite{glassner2014principles}{11}
\bibcite{stanford_graphics}{12}
\bibcite{jensen1996global}{13}
\bibcite{kajiya1986rendering}{14}
\bibcite{keller2016path}{15}
\bibcite{krivanek2014recent}{16}
\bibcite{morokoff1995quasi}{17}
\bibcite{nvidia_turing_architecture_whitepaper_2018}{18}
\bibcite{pan2006virtual}{19}
\bibcite{pegoraro2008towards}{20}
\bibcite{ramamoorthi2012theory}{21}
\bibcite{scratchapixel_2015}{22}
\bibcite{sutton2011reinforcement}{23}
\bibcite{normals}{24}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}An Example Appendix}{25}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appx:example}{{A}{25}{An Example Appendix}{appendix.A}{}}
