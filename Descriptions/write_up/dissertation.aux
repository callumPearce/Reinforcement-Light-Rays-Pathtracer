\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\providecommand \oddpage@label [2]{}
\citation{dahm2017learning}
\citation{christensen2016path}
\citation{kajiya1986rendering}
\citation{keller2016path}
\citation{stanford_graphics}
\citation{krivanek2014recent}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:context}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Monte Carlo Path Tracing for Light Transport Simulation}{1}{section.1.1}}
\newlabel{sec:conceptual_path_trace}{{1.1}{1}{Monte Carlo Path Tracing for Light Transport Simulation}{section.1.1}{}}
\citation{dahm2017learning}
\citation{jensen1996global}
\citation{keller2016path}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An illustration of path tracing, where three light paths are traced from the camera through a pixel, to the light source in a simple 3-dimensional scene. The light paths are used to determine the pixel colours of the rendered image.\relax }}{2}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:path_tracing_overview}{{1.1}{2}{An illustration of path tracing, where three light paths are traced from the camera through a pixel, to the light source in a simple 3-dimensional scene. The light paths are used to determine the pixel colours of the rendered image.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Two renders of the Cornell Box, where the left is directly illuminated and the right is globally illuminated.\relax }}{2}{figure.caption.8}}
\newlabel{fig:direct_and_global}{{1.2}{2}{Two renders of the Cornell Box, where the left is directly illuminated and the right is globally illuminated.\relax }{figure.caption.8}{}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\citation{sutton2011reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Two renders of a simple room using 16 SPP. Where one does not use importance sampling in the construction of light paths (left), and the other does so based on a reinforcement learning rule \cite  {dahm2017learning} (right). A clear reduction in image noise can be seen by the use of importance sampling.\relax }}{3}{figure.caption.9}}
\newlabel{fig:noise_reduction_simple_room}{{1.3}{3}{Two renders of a simple room using 16 SPP. Where one does not use importance sampling in the construction of light paths (left), and the other does so based on a reinforcement learning rule \cite {dahm2017learning} (right). A clear reduction in image noise can be seen by the use of importance sampling.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Temporal Difference Learning for Importance Sampling Ray Directions}{3}{section.1.2}}
\newlabel{sec:td_learn_for_importance}{{1.2}{3}{Temporal Difference Learning for Importance Sampling Ray Directions}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}What is Temporal Difference learning?}{3}{subsection.1.2.1}}
\citation{dahm2017learning}
\citation{ramamoorthi2012theory}
\citation{dahm2017learning}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Temporal Difference learning methods for Efficient Light Transport Simulation}{4}{subsection.1.2.2}}
\newlabel{sec:temp_diff_for_light_transport}{{1.2.2}{4}{Temporal Difference learning methods for Efficient Light Transport Simulation}{subsection.1.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Why use Temporal Difference Learning for Importance Sampling?}{4}{subsection.1.2.3}}
\newlabel{sec:why_temp_diff}{{1.2.3}{4}{Why use Temporal Difference Learning for Importance Sampling?}{subsection.1.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces An illustration of a light blocker for an importance sampling scheme which does not consider visibility. Each arrow represents a possible direction the light path will be continued in. Clearly the reflected light path is likely to hit the blocker, reducing its contribution to the approximation of a pixel value.\relax }}{4}{figure.caption.10}}
\newlabel{fig:blocker}{{1.4}{4}{An illustration of a light blocker for an importance sampling scheme which does not consider visibility. Each arrow represents a possible direction the light path will be continued in. Clearly the reflected light path is likely to hit the blocker, reducing its contribution to the approximation of a pixel value.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}From Discrete to a Continuous State Space}{4}{subsection.1.2.4}}
\newlabel{sec:discrete_to_continuous_motivation}{{1.2.4}{4}{From Discrete to a Continuous State Space}{subsection.1.2.4}{}}
\citation{kajiya1986rendering}
\citation{goral1984modeling}
\citation{whitted2005improved}
\citation{jensen1996global}
\citation{wald2001state}
\citation{pan2006virtual}
\citation{bloomberg.com}
\citation{nvidia_turing_architecture_whitepaper_2018}
\citation{bako2017kernel}
\citation{chaitanya2017interactive}
\citation{christensen2018renderman}
\citation{georgiev2018arnold}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Motivation}{5}{section.1.3}}
\newlabel{sec:motivation}{{1.3}{5}{Motivation}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Real time Rendering using Accurate Light Transport Simulation}{5}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Recent Developments}{5}{subsection.1.3.2}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Objectives and Challenges}{6}{section.1.4}}
\citation{morokoff1995quasi}
\citation{morokoff1995quasi}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Monte Carlo Integration, Path Tracing, and Temporal Difference Learning}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:technical}{{2}{7}{Monte Carlo Integration, Path Tracing, and Temporal Difference Learning}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Monte Carlo Integration and Importance Sampling}{7}{section.2.1}}
\newlabel{sec:monte_carlo}{{2.1}{7}{Monte Carlo Integration and Importance Sampling}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Monte Carlo Integration}{7}{subsection.2.1.1}}
\newlabel{sec:monte_carlo_approx}{{2.1.1}{7}{Monte Carlo Integration}{subsection.2.1.1}{}}
\newlabel{eq:integral}{{2.1}{7}{Monte Carlo Integration}{equation.2.1.1}{}}
\newlabel{eq:monte_carlo}{{2.2}{7}{Monte Carlo Integration}{equation.2.1.2}{}}
\newlabel{eq:generalized_mc}{{2.3}{7}{Monte Carlo Integration}{equation.2.1.3}{}}
\citation{bellman1966dynamic}
\citation{morokoff1995quasi}
\newlabel{eq:unbiased}{{2.4}{8}{Monte Carlo Integration}{equation.2.1.4}{}}
\newlabel{eq:law_large_numbers}{{2.5}{8}{Monte Carlo Integration}{equation.2.1.5}{}}
\newlabel{eq:sample_variance}{{2.6}{8}{Monte Carlo Integration}{equation.2.1.6}{}}
\newlabel{eq:mc_error}{{2.7}{8}{Monte Carlo Integration}{equation.2.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Importance Sampling for Reducing Approximation Variance}{8}{subsection.2.1.2}}
\newlabel{sec:importance_sampling}{{2.1.2}{8}{Importance Sampling for Reducing Approximation Variance}{subsection.2.1.2}{}}
\newlabel{eq:constant_monte_carlo}{{2.8}{8}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.8}{}}
\citation{scratchapixel_2015}
\citation{kajiya1986rendering}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Constant function  with a sample point\relax }}{9}{figure.caption.11}}
\newlabel{fig:constant_function}{{2.1}{9}{Constant function\\ with a sample point\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Non-linear function  with a sample point\relax }}{9}{figure.caption.11}}
\newlabel{fig:non_lin_function}{{2.2}{9}{Non-linear function\\ with a sample point\relax }{figure.caption.11}{}}
\newlabel{eq:constant_conversion}{{2.9}{9}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.9}{}}
\newlabel{eq:solve_mc_integration}{{2.10}{9}{Importance Sampling for Reducing Approximation Variance}{equation.2.1.10}{}}
\citation{dutre2004state}
\newlabel{fig:improtance_correct}{{2.3a}{10}{Importance sampling reducing variance\relax }{figure.caption.12}{}}
\newlabel{sub@fig:improtance_correct}{{a}{10}{Importance sampling reducing variance\relax }{figure.caption.12}{}}
\newlabel{fig:improtance_uniform}{{2.3b}{10}{Uniform\\ sampling\relax }{figure.caption.12}{}}
\newlabel{sub@fig:improtance_uniform}{{b}{10}{Uniform\\ sampling\relax }{figure.caption.12}{}}
\newlabel{fig:improtance_incorrect}{{2.3c}{10}{Importance sampling increasing variance\relax }{figure.caption.12}{}}
\newlabel{sub@fig:improtance_incorrect}{{c}{10}{Importance sampling increasing variance\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Graphical representation of a function $f(x)$ (red) and the corresponding PDF $pdf(x)$ (blue) used in the MC integration approximation for the integral of $f(x)$.\relax }}{10}{figure.caption.12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Monte Carlo Path Tracing}{10}{section.2.2}}
\newlabel{sec:mc_pathtracing}{{2.2}{10}{Monte Carlo Path Tracing}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Rendering Equation}{10}{subsection.2.2.1}}
\newlabel{sec:rendering_equation}{{2.2.1}{10}{The Rendering Equation}{subsection.2.2.1}{}}
\newlabel{eq:rendering_equation}{{2.11}{10}{The Rendering Equation}{equation.2.2.11}{}}
\citation{glassner2014principles}
\citation{normals}
\citation{glassner2014principles}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(x,\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(x, \omega _i)$ is identical to calculating the outgoing radiance $L_o(h(x, \omega _i), -\omega _i)$ as we assume no radiance is lost along a ray line, hence $L_o, L_i$ are recursive functions.\relax }}{11}{figure.caption.13}}
\newlabel{fig:recursive_rendering}{{2.4}{11}{A diagrammatic representation of the recursive nature of the rendering equation. The outgoing radiance ($L_o$) in a given direction $\omega $ from a point $x$ requires an estimation of the incident radiance coming from all angles in the hemisphere around the point, that is $L_i(x,\omega _i)$ $\forall \omega _i \in \Omega $. To calculate $L_i(x, \omega _i)$ is identical to calculating the outgoing radiance $L_o(h(x, \omega _i), -\omega _i)$ as we assume no radiance is lost along a ray line, hence $L_o, L_i$ are recursive functions.\relax }{figure.caption.13}{}}
\newlabel{fig:diffuse_brdf}{{2.5a}{11}{Diffuse BRDF\relax }{figure.caption.14}{}}
\newlabel{sub@fig:diffuse_brdf}{{a}{11}{Diffuse BRDF\relax }{figure.caption.14}{}}
\newlabel{fig:specular_brdf}{{2.5b}{11}{Specular BRDF\relax }{figure.caption.14}{}}
\newlabel{sub@fig:specular_brdf}{{b}{11}{Specular BRDF\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A representation of both a diffuse surface and specular surface BRDF's for a given angle of incidence $\omega '$. The surface point is located where all outgoing arrows begin. The arrows indicate a subset of directions possible for the incident ray to be reflected in. All possible directions an incident ray be reflected in are defined by any vector which can be formed from the surface point to the curved line for an incident direction $\omega '$. The further away a point is on the curved line, the more likely a ray is to reflected in a direction from the surface point to that point on the curved line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favours a small subset of directions in the hemisphere surrounding the surface point.\relax }}{11}{figure.caption.14}}
\newlabel{fig:brdfs}{{2.5}{11}{A representation of both a diffuse surface and specular surface BRDF's for a given angle of incidence $\omega '$. The surface point is located where all outgoing arrows begin. The arrows indicate a subset of directions possible for the incident ray to be reflected in. All possible directions an incident ray be reflected in are defined by any vector which can be formed from the surface point to the curved line for an incident direction $\omega '$. The further away a point is on the curved line, the more likely a ray is to reflected in a direction from the surface point to that point on the curved line. The diffuse surface is equally likely to reflect a ray in any direction. Whereas, the specular surface favours a small subset of directions in the hemisphere surrounding the surface point.\relax }{figure.caption.14}{}}
\citation{stanford_graphics}
\citation{christensen2016path}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Two sculptures, one made from a diffuse material (a) and the other from a specular material (b). The specular sculpture appearance is dependent upon the viewing angle, whereas the diffuse sculpture is not.\relax }}{12}{figure.caption.15}}
\newlabel{fig:material_pics}{{2.6}{12}{Two sculptures, one made from a diffuse material (a) and the other from a specular material (b). The specular sculpture appearance is dependent upon the viewing angle, whereas the diffuse sculpture is not.\relax }{figure.caption.15}{}}
\newlabel{eq:conservation_energy}{{2.12}{12}{The Rendering Equation}{equation.2.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Path Tracing}{12}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Path Tracing}{12}{section*.16}}
\newlabel{sec:monte_carlo_path_tracing}{{2.2.2}{12}{Monte Carlo Path Tracing}{section*.16}{}}
\newlabel{eq:rendering_eq_monte_carlo}{{2.13}{12}{Monte Carlo Path Tracing}{equation.2.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces An indirectly illuminated scene rendered by path tracing. The grid of image sections show how increasing the number of SPP reduces image noise. Beginning in the top left with 16 SPP, to the bottom right with 512 SPP by doubling the number of SPP each time. The full image on the right is a reference image with 4096 SPP where the MC approximation has almost converged for all pixel values.\relax }}{13}{figure.caption.17}}
\newlabel{fig:reduce_noise_spp_example}{{2.7}{13}{An indirectly illuminated scene rendered by path tracing. The grid of image sections show how increasing the number of SPP reduces image noise. Beginning in the top left with 16 SPP, to the bottom right with 512 SPP by doubling the number of SPP each time. The full image on the right is a reference image with 4096 SPP where the MC approximation has almost converged for all pixel values.\relax }{figure.caption.17}{}}
\newlabel{alg:forward_path_tracing}{{1}{13}{Monte Carlo Path Tracing}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pseudo code for a path tracer. Given a camera position, scene geometry, this algorithm will render a single image by finding the colour estimate for each pixel using Monte Carlo path tracing. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{13}{algocf.1}}
\citation{bashford2012significance}
\citation{cline2008table}
\citation{pegoraro2008towards}
\citation{dahm2017learning}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{Importance Sampling in Path Tracing}{14}{section*.18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reinforcement Learning and TD-Learning}{14}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Markov Decision Processes}{14}{subsection.2.3.1}}
\citation{introToRL}
\citation{sutton2011reinforcement}
\citation{introToRL}
\citation{sutton2011reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Markov Decision Process \cite  {sutton2011reinforcement}\relax }}{15}{figure.caption.19}}
\newlabel{fig:mdp}{{2.8}{15}{Markov Decision Process \cite {sutton2011reinforcement}\relax }{figure.caption.19}{}}
\newlabel{eq:markov_property}{{2.14}{15}{Markov Decision Processes}{equation.2.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Goals and Rewards}{15}{subsection.2.3.2}}
\newlabel{eq:return}{{2.15}{15}{Goals and Rewards}{equation.2.3.15}{}}
\citation{sutton2011reinforcement}
\citation{sutton1996generalization}
\citation{konidaris2011value}
\citation{uther1998tree}
\citation{mnih2013playing}
\citation{model_free_prediction}
\citation{sutton2011reinforcement}
\citation{mdp_dynamic_prog}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Value Function and Optimality}{16}{subsection.2.3.3}}
\newlabel{sec:optimal_value}{{2.3.3}{16}{Value Function and Optimality}{subsection.2.3.3}{}}
\newlabel{eq:value_function}{{2.16}{16}{Value Function and Optimality}{equation.2.3.16}{}}
\newlabel{eq:optimal_value}{{2.17}{16}{Value Function and Optimality}{equation.2.3.17}{}}
\newlabel{eq:bellman_optimal}{{2.18}{16}{Value Function and Optimality}{equation.2.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Temporal Difference Learning}{16}{subsection.2.3.4}}
\newlabel{sec:td_learning}{{2.3.4}{16}{Temporal Difference Learning}{subsection.2.3.4}{}}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\newlabel{eq:sarsa}{{2.19}{17}{Sarsa}{equation.2.3.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Q-Learning}{17}{section*.21}}
\newlabel{eq:q_learning}{{2.20}{17}{Q-Learning}{equation.2.3.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{17}{section*.22}}
\citation{exploration_vs_exploitation}
\citation{sutton2011reinforcement}
\newlabel{eq:expected_sarsa}{{2.21}{18}{Expected Sarsa}{equation.2.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Exploration vs Exploitation}{18}{subsection.2.3.5}}
\newlabel{sec:exploration_vs_exploitation}{{2.3.5}{18}{Exploration vs Exploitation}{subsection.2.3.5}{}}
\citation{dahm2017learning}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}The Expected Sarsa and Neural-Q Path Tracers}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:td_deep_sampling}{{3}{19}{The Expected Sarsa and Neural-Q Path Tracers}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Expected Sarsa Path Tracer}{19}{section.3.1}}
\newlabel{sec:expecte_sarsa_path_tracer}{{3.1}{19}{The Expected Sarsa Path Tracer}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Linking TD-Learning and Light Transport Simulation}{19}{subsection.3.1.1}}
\newlabel{sec:td_light_transport}{{3.1.1}{19}{Linking TD-Learning and Light Transport Simulation}{subsection.3.1.1}{}}
\newlabel{eq:sarsa_integral}{{3.3}{19}{Linking TD-Learning and Light Transport Simulation}{equation.3.1.3}{}}
\citation{greger1998irradiance}
\citation{greger1998irradiance}
\newlabel{eq:expected_sarsa_td_learning}{{3.4}{20}{Linking TD-Learning and Light Transport Simulation}{equation.3.1.4}{}}
\newlabel{eq:mc_expected_sarsa_td_learning}{{3.5}{20}{Linking TD-Learning and Light Transport Simulation}{equation.3.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An illustration of a simple incident radiance distribution for a given point $x$ in a scene. Where $\omega _i$ $\forall \omega _i \in \Omega $ are all possible incident directions in the hemisphere around the point $x$ oriented to the surface normal at $x$.The scene contains two area lights with the same power which are visible from $x$ in the scene, causing two equal level peaks in the incident radiance distribution for $x$.\relax }}{20}{figure.caption.24}}
\newlabel{fig:incident_radiance_distribution}{{3.1}{20}{An illustration of a simple incident radiance distribution for a given point $x$ in a scene. Where $\omega _i$ $\forall \omega _i \in \Omega $ are all possible incident directions in the hemisphere around the point $x$ oriented to the surface normal at $x$.The scene contains two area lights with the same power which are visible from $x$ in the scene, causing two equal level peaks in the incident radiance distribution for $x$.\relax }{figure.caption.24}{}}
\citation{shirley1994notes}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}The Irradiance Volume}{21}{subsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An Irradiance Volume. Each sector holds the incoming radiance $L_i(x,\omega _k)$, the more green a sector is the lower the stored radiance in that sector, the more red a sector is the higher the stored radiance in that sector. \relax }}{21}{figure.caption.25}}
\newlabel{fig:irradiance_volume}{{3.2}{21}{An Irradiance Volume. Each sector holds the incoming radiance $L_i(x,\omega _k)$, the more green a sector is the lower the stored radiance in that sector, the more red a sector is the higher the stored radiance in that sector. \relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces An example of discretizing location in the scene into Irradiance Volume locations. The geometry mesh (a) is used to uniformly sample Irradiance volume positions. Image (b) shows a voronoi plot for the Irradiance Volumes in the scene, where each pixel is coloured to the represent its closest Irradiance Volume, so each sector of colour in (b) represents a different Irradiance Volume location. Finally (c) gives a render using the Expected Sarsa path tracer based on Algorithm \ref  {alg:expected_sarsa_pathtracer}.\relax }}{21}{figure.caption.26}}
\newlabel{fig:scene_discretization_example}{{3.3}{21}{An example of discretizing location in the scene into Irradiance Volume locations. The geometry mesh (a) is used to uniformly sample Irradiance volume positions. Image (b) shows a voronoi plot for the Irradiance Volumes in the scene, where each pixel is coloured to the represent its closest Irradiance Volume, so each sector of colour in (b) represents a different Irradiance Volume location. Finally (c) gives a render using the Expected Sarsa path tracer based on Algorithm \ref {alg:expected_sarsa_pathtracer}.\relax }{figure.caption.26}{}}
\citation{bentley1975multidimensional}
\citation{devroye2006nonuniform}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Expected Sarsa Path Tracing}{22}{subsection.3.1.3}}
\newlabel{sec:expected_sarsa_path_tracer}{{3.1.3}{22}{Expected Sarsa Path Tracing}{subsection.3.1.3}{}}
\citation{dahm2017learning}
\newlabel{alg:expected_sarsa_pathtracer}{{2}{23}{Addition 3: Update Irradiance Volume Distributions}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Expected Sarsa path tracer pseudo code following NVIDIA's method in \cite  {dahm2017learning}. Given a camera position, scene geometry, this algorithm will render a single image using a tabular Expected Sarsa approach to progressively reduce image noise. Where $N$ is the pre-specified number of sampled light paths per pixel.\relax }}{23}{algocf.2}}
\newlabel{eq:mc_expected_sarsa_pdf}{{3.6}{23}{Monte Carlo Integration}{equation.3.1.6}{}}
\citation{dahm2017learning}
\newlabel{fig:uniform_pdf}{{3.4a}{24}{Uniform radiance distribution\relax }{figure.caption.31}{}}
\newlabel{sub@fig:uniform_pdf}{{a}{24}{Uniform radiance distribution\relax }{figure.caption.31}{}}
\newlabel{fig:not_uniform_pdf}{{3.4b}{24}{Non-uniform radiance distribution\relax }{figure.caption.31}{}}
\newlabel{sub@fig:not_uniform_pdf}{{b}{24}{Non-uniform radiance distribution\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A 2-dimensional view of a subset of values from two radiance distributions. One for a unit hemisphere (left) with a radiance distribution. One for an Irradiance Volume (right) with non-uniform radiance distribution. Where the arrows represent sampled directions and the values at the end of the arrows are the radiance distribution evaluated in the associated directions.\relax }}{24}{figure.caption.31}}
\newlabel{fig:pdfs}{{3.4}{24}{A 2-dimensional view of a subset of values from two radiance distributions. One for a unit hemisphere (left) with a radiance distribution. One for an Irradiance Volume (right) with non-uniform radiance distribution. Where the arrows represent sampled directions and the values at the end of the arrows are the radiance distribution evaluated in the associated directions.\relax }{figure.caption.31}{}}
\newlabel{eq:decay_lr}{{3.7}{24}{Consistency}{equation.3.1.7}{}}
\citation{deep_rl_function_approx}
\citation{sutton2011reinforcement}
\citation{sutton1996generalization}
\citation{konidaris2011value}
\citation{uther1998tree}
\citation{lecun2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Neural-Q Path Tracer}{25}{section.3.2}}
\newlabel{sec:neural_q_path_tracer}{{3.2}{25}{The Neural-Q Path Tracer}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to Deep Reinforcement Learning}{25}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Value Function Approximation}{25}{section*.33}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{25}{section*.34}}
\newlabel{eq:example_loss}{{3.8}{25}{Stochastic Gradient Descent}{equation.3.2.8}{}}
\citation{deep_rl_function_approx}
\citation{lillicrap2015continuous}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsubsection}{Bootstrapping}{26}{section*.35}}
\newlabel{sec:bootstrapping}{{3.2.1}{26}{Bootstrapping}{section*.35}{}}
\newlabel{eq:td_error_q_learning}{{3.9}{26}{Bootstrapping}{equation.3.2.9}{}}
\newlabel{eq:loss_q_learning}{{3.10}{26}{Bootstrapping}{equation.3.2.10}{}}
\newlabel{eq:q_learning_derv_loss}{{3.11}{26}{Bootstrapping}{equation.3.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Deep Reinforcement Learning Motivation}{26}{subsection.3.2.2}}
\newlabel{sec:deep_rl_motivation}{{3.2.2}{26}{Deep Reinforcement Learning Motivation}{subsection.3.2.2}{}}
\citation{mnih2013playing}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Deep Q-learning for Light Transport}{27}{subsection.3.2.3}}
\newlabel{eq:neural_q_loss}{{3.12}{27}{Deep Q-learning for Light Transport}{equation.3.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Artificial Neural Network Architecture}{27}{subsection.3.2.4}}
\newlabel{sec:ann_architecture}{{3.2.4}{27}{Artificial Neural Network Architecture}{subsection.3.2.4}{}}
\citation{mnih2013playing}
\citation{nair2010rectified}
\citation{ren2013global}
\citation{sutton2011reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An illustration of the process for a forward pass on the ANN used for the Neural-Q path tracer. Starting with the scene on the left, all vertices in the scene (yellow points) are converted into a coordinate system relative to the red point $x$, producing $\mathbf  {v^x}$. This is passed to the input layer of the ANN which then computes a forward pass through the two hidden layers fully connected layers, each with a width of $200$ neurons. The output layers width is equal to the number of discrete incident directions $m$ used in the approximation of the incident radiance function. Each output represents the approximated incident radiance on the position $x$ for every discrete incident direction $\mathaccentV {hat}05E{q_\theta }(x, \omega _k)$ $\forall k = 1, ..., m$, using parameter values $\theta $.\relax }}{28}{figure.caption.36}}
\newlabel{fig:nn_radiance_estimate_illustration}{{3.5}{28}{An illustration of the process for a forward pass on the ANN used for the Neural-Q path tracer. Starting with the scene on the left, all vertices in the scene (yellow points) are converted into a coordinate system relative to the red point $x$, producing $\mathbf {v^x}$. This is passed to the input layer of the ANN which then computes a forward pass through the two hidden layers fully connected layers, each with a width of $200$ neurons. The output layers width is equal to the number of discrete incident directions $m$ used in the approximation of the incident radiance function. Each output represents the approximated incident radiance on the position $x$ for every discrete incident direction $\hat {q_\theta }(x, \omega _k)$ $\forall k = 1, ..., m$, using parameter values $\theta $.\relax }{figure.caption.36}{}}
\citation{kingma2014adam}
\citation{deep_rl_function_approx}
\citation{lillicrap2015continuous}
\citation{shirley1994notes}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Neural-Q Path Tracing Algorithm}{29}{subsection.3.2.5}}
\citation{mnih2013playing}
\citation{sutton2011reinforcement}
\citation{sutton2011reinforcement}
\citation{muller2018neural}
\newlabel{alg:neural_q_pathtracer}{{\caption@xref {alg:neural_q_pathtracer}{ on input line 397}}{31}{Addition 4: Decaying decaying $\epsilon $-greedy}{algocf.caption.41}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Neural-Q forward path tracer. Given a camera position, scene geometry, epsilon and epsilon decay, this algorithm will render a single image. Where $N$ is the pre-specified number of sampled light paths per pixel. The algorithm trains the ANN online to progressively reduce noise in consecutive rendered images, by improving its approximation of the incident radiance function for the scene.\relax }}{31}{algocf.3}}
\citation{glm}
\citation{sdl2}
\citation{dynet}
\citation{cuda}
\citation{tensorflow2015-whitepaper}
\citation{maya}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Comparing the Expected Sarsa and Neural-Q Path Tracers}{32}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:evaluation}{{4}{32}{Comparing the Expected Sarsa and Neural-Q Path Tracers}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Setup}{32}{section.4.1}}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Assessing the reduction in image Noise for Monte Carlo Path Tracing}{33}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Quantifying the Reduction in Image Noise}{33}{subsection.4.2.1}}
\newlabel{eq:mape}{{4.1}{33}{Quantifying the Reduction in Image Noise}{equation.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A comparison of the default forward path tracer, Expected Sarsa path tracer, and the Neural-Q path tracer of their rendered image noise for four different rendered scenes. All images were rendered with 128 SPP, apart from the reference images in the very right column which use 4096 SPP. The score under each column in an image row corresponds to the MAPE score for each path tracing algorithm for the particular scene.The Neural-Q and Expected Sarsa algorithms both used $144$ discrete directions in each discretized hemisphere to estimate the incident radiance distribution at a given point. The Neural-Q path tracer used the ANN architecture described in \ref  {sec:ann_architecture} for all fours scenes with a decaying $\epsilon $-greedy policy starting at $\epsilon =1$, with a decay of $\delta = 0.05$ applied after every pixel in the image has had a light path sampled through it. The Expected Sarsa path tracer used just enough Irradiance Volumes (which varied depending on the scene) to facilitate a significant reduction in image noise in all four renders, such that it was competitive with the Neural-Q path tracer. See appendix \ref  {appx:images} for the full images produced by all three path tracers.\relax }}{35}{figure.caption.42}}
\newlabel{fig:mape_results_grid}{{4.1}{35}{A comparison of the default forward path tracer, Expected Sarsa path tracer, and the Neural-Q path tracer of their rendered image noise for four different rendered scenes. All images were rendered with 128 SPP, apart from the reference images in the very right column which use 4096 SPP. The score under each column in an image row corresponds to the MAPE score for each path tracing algorithm for the particular scene.The Neural-Q and Expected Sarsa algorithms both used $144$ discrete directions in each discretized hemisphere to estimate the incident radiance distribution at a given point. The Neural-Q path tracer used the ANN architecture described in \ref {sec:ann_architecture} for all fours scenes with a decaying $\epsilon $-greedy policy starting at $\epsilon =1$, with a decay of $\delta = 0.05$ applied after every pixel in the image has had a light path sampled through it. The Expected Sarsa path tracer used just enough Irradiance Volumes (which varied depending on the scene) to facilitate a significant reduction in image noise in all four renders, such that it was competitive with the Neural-Q path tracer. See appendix \ref {appx:images} for the full images produced by all three path tracers.\relax }{figure.caption.42}{}}
\citation{christensen2016path}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}A Closer Inspection of Pixel Error Values}{36}{subsection.4.2.2}}
\newlabel{sec:close_inspec_pixels}{{4.2.2}{36}{A Closer Inspection of Pixel Error Values}{subsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Histograms for the average RGB pixel error values for all four rendered scenes using both the Expected Sarsa and the Neural-Q path tracers. Where the average RGB pixel error value is the average difference in all RGB colour channels between a pixel in the reference image, and the corresponding pixel in the image rendered by either the Expected Sarsa or Neural-Q path tracers. The max average RGB pixel error value is $255$, which corresponds to the case where the reference images pixels value is $(255,255,255)$ whereas the rendered images was $(0,0,0)$ or vice versa. The histograms are based on the rendered images presented in Figure \ref  {fig:mape_results_grid}.\relax }}{36}{figure.caption.43}}
\newlabel{fig:histogram_errors}{{4.2}{36}{Histograms for the average RGB pixel error values for all four rendered scenes using both the Expected Sarsa and the Neural-Q path tracers. Where the average RGB pixel error value is the average difference in all RGB colour channels between a pixel in the reference image, and the corresponding pixel in the image rendered by either the Expected Sarsa or Neural-Q path tracers. The max average RGB pixel error value is $255$, which corresponds to the case where the reference images pixels value is $(255,255,255)$ whereas the rendered images was $(0,0,0)$ or vice versa. The histograms are based on the rendered images presented in Figure \ref {fig:mape_results_grid}.\relax }{figure.caption.43}{}}
\newlabel{fig:true_incident_distribution}{{4.3a}{37}{Ground Truth\relax }{figure.caption.44}{}}
\newlabel{sub@fig:true_incident_distribution}{{a}{37}{Ground Truth\relax }{figure.caption.44}{}}
\newlabel{fig:exp_sarsa_distribution}{{4.3b}{37}{Expected Sarsa\relax }{figure.caption.44}{}}
\newlabel{sub@fig:exp_sarsa_distribution}{{b}{37}{Expected Sarsa\relax }{figure.caption.44}{}}
\newlabel{fig:neural_q_distribution}{{4.3c}{37}{Neural-Q\relax }{figure.caption.44}{}}
\newlabel{sub@fig:neural_q_distribution}{{c}{37}{Neural-Q\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualisations of the incident radiance distribution for three different positions in a scene. The incident radiance is represented by placing an adaptive quadrature at each point with 144 different sectors, each representing an incident direction which the incident radiance has been calculated at. The positions are in front of a single large area light within a simple scene. (a) presents the ground truth incident radiance distribution, which has been calculated by firing 4096 sampled light paths per sector and averaging their incident radiance approximation. (b) The approximated incident radiance distribution for the three positions produced by the Expected Sarsa method when trained 1024 SPP, and (c) is the same but for the Neural-Q method trained with 1024 SPP. Due to the single large area light in front of the point, most of the incident radiance on the points will come from directions spanning from the area light on the left.\relax }}{37}{figure.caption.44}}
\newlabel{fig:distribution_visualisation}{{4.3}{37}{Visualisations of the incident radiance distribution for three different positions in a scene. The incident radiance is represented by placing an adaptive quadrature at each point with 144 different sectors, each representing an incident direction which the incident radiance has been calculated at. The positions are in front of a single large area light within a simple scene. (a) presents the ground truth incident radiance distribution, which has been calculated by firing 4096 sampled light paths per sector and averaging their incident radiance approximation. (b) The approximated incident radiance distribution for the three positions produced by the Expected Sarsa method when trained 1024 SPP, and (c) is the same but for the Neural-Q method trained with 1024 SPP. Due to the single large area light in front of the point, most of the incident radiance on the points will come from directions spanning from the area light on the left.\relax }{figure.caption.44}{}}
\newlabel{fig:dist_expected_sarsa}{{4.4a}{37}{Expected Sarsa\relax }{figure.caption.45}{}}
\newlabel{sub@fig:dist_expected_sarsa}{{a}{37}{Expected Sarsa\relax }{figure.caption.45}{}}
\newlabel{fig:dist_neural_q}{{4.4b}{37}{Neural-Q\relax }{figure.caption.45}{}}
\newlabel{sub@fig:dist_neural_q}{{b}{37}{Neural-Q\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An illustration of the incident radiance distribution for a given point directly in front of the light source shown in figure \ref  {fig:distribution_visualisation} for both the Expected Sarsa and Neural-Q methods. Where $\omega _i \in \Omega $ are discrete incident angles on the point, and $p(\omega _i)$ is the probability density function over incident radiance evaluated at angle $\omega _i$. The red line represents the true incident radiance distribution on the point, the blue represents the corresponding methods approximation of the incident radiance distribution on the point.\relax }}{37}{figure.caption.45}}
\newlabel{fig:dist_2_methods}{{4.4}{37}{An illustration of the incident radiance distribution for a given point directly in front of the light source shown in figure \ref {fig:distribution_visualisation} for both the Expected Sarsa and Neural-Q methods. Where $\omega _i \in \Omega $ are discrete incident angles on the point, and $p(\omega _i)$ is the probability density function over incident radiance evaluated at angle $\omega _i$. The red line represents the true incident radiance distribution on the point, the blue represents the corresponding methods approximation of the incident radiance distribution on the point.\relax }{figure.caption.45}{}}
\citation{dahm2017learning}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}From a Discrete to Continuous State Space}{38}{section.4.3}}
\citation{dahm2017learning}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Reference Images of the Door Room and Shelter scenes next to a 1SPP render using an Expected Sarsa path tracer and a Neural-Q path tracer, both trained for 100 epochs. The single sample for each pixel in the Expected Sarsa and Neural-Q renders is constructed by reflecting the light path in the direction of highest estimated radiance upon every intersection point, until the light is intersected with.\relax }}{39}{figure.caption.46}}
\newlabel{fig:1_spp_max_dir}{{4.5}{39}{Reference Images of the Door Room and Shelter scenes next to a 1SPP render using an Expected Sarsa path tracer and a Neural-Q path tracer, both trained for 100 epochs. The single sample for each pixel in the Expected Sarsa and Neural-Q renders is constructed by reflecting the light path in the direction of highest estimated radiance upon every intersection point, until the light is intersected with.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Convergence}{39}{section.4.4}}
\newlabel{sec:convergence_learning_incident}{{4.4}{39}{Convergence}{section.4.4}{}}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{hyperion}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Training curves for the average path length and number of zero contribution light paths when rendering the Shelter scene for 100 epochs, as well as the 1st and 100th rendered frames when using 1 SPP for both images. This is shown for both the Expected Sarsa and Neural-Q path tracers. An epoch represents one sampled light path through every pixel in the image. The average path length is the number reflections a light path takes before intersecting with a light source. A zero contribution light path is one which contributes almost zero colour to the final image.\relax }}{40}{figure.caption.47}}
\newlabel{fig:training_curves_archway}{{4.6}{40}{Training curves for the average path length and number of zero contribution light paths when rendering the Shelter scene for 100 epochs, as well as the 1st and 100th rendered frames when using 1 SPP for both images. This is shown for both the Expected Sarsa and Neural-Q path tracers. An epoch represents one sampled light path through every pixel in the image. The average path length is the number reflections a light path takes before intersecting with a light source. A zero contribution light path is one which contributes almost zero colour to the final image.\relax }{figure.caption.47}{}}
\citation{dahm2017learning}
\citation{devastator}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}From Memory Bound to Compute Bound}{41}{section.4.5}}
\newlabel{sec:mem_to_comp}{{4.5}{41}{From Memory Bound to Compute Bound}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Memory Usage}{41}{subsection.4.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Additional memory required in Megabytes (MB) to render each of the four scenes shown in Figure \ref  {fig:mape_results_grid} using both the Expected Sarsa and Neural-Q path tracers.\relax }}{41}{table.caption.48}}
\newlabel{tab:memory_usage}{{4.1}{41}{Additional memory required in Megabytes (MB) to render each of the four scenes shown in Figure \ref {fig:mape_results_grid} using both the Expected Sarsa and Neural-Q path tracers.\relax }{table.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{41}{section*.49}}
\citation{ren2013global}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The Complex Pillars scene rendered using two Expected Sarsa path tracers, both trained for 1000 SPP and the rendered images in the middle column are produced using 128 SPP. The first row presents the Expected Sarsa path tracer with 36029 Irradiance Volumes. The second row is the Expected Sarsa path tracer with only 780 Irradiance Volumes. The left column is a voronoi plot of the Irradiance Volumes, where each pixel is given a colour to represent which Irradiance Volume it is closest to. The MAPE score and render is given for both renders in the middle column. The right column presents two close-ups of the corresponding image in the middle column to give a visual comparison of the noise between the two images.\relax }}{42}{figure.caption.50}}
\newlabel{fig:voronoi_difference}{{4.7}{42}{The Complex Pillars scene rendered using two Expected Sarsa path tracers, both trained for 1000 SPP and the rendered images in the middle column are produced using 128 SPP. The first row presents the Expected Sarsa path tracer with 36029 Irradiance Volumes. The second row is the Expected Sarsa path tracer with only 780 Irradiance Volumes. The left column is a voronoi plot of the Irradiance Volumes, where each pixel is given a colour to represent which Irradiance Volume it is closest to. The MAPE score and render is given for both renders in the middle column. The right column presents two close-ups of the corresponding image in the middle column to give a visual comparison of the noise between the two images.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q}{42}{section*.52}}
\citation{crockett1995parallel}
\citation{alerstam2008parallel}
\citation{fatahalian2009data}
\citation{embarissingly_parallelizable}
\citation{accelerated_ray_tracing}
\citation{hyperion}
\citation{raynal2012concurrent}
\citation{cuda}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Cornell Box scene rendered using 1MB of memory to store the Irradiance Volumes for the Expected Sarsa path tracer. The large black artefacts and incorrect colouring of the red block is a result of using too few Irradiance Volumes to approximate the incident radiance function $L_i(x, \omega )$. Causing importance sampling to actually increase the variance in pixel value estimates, leading to an increase in overall image noise.\relax }}{43}{figure.caption.51}}
\newlabel{fig:incorrect_cornell}{{4.8}{43}{Cornell Box scene rendered using 1MB of memory to store the Irradiance Volumes for the Expected Sarsa path tracer. The large black artefacts and incorrect colouring of the red block is a result of using too few Irradiance Volumes to approximate the incident radiance function $L_i(x, \omega )$. Causing importance sampling to actually increase the variance in pixel value estimates, leading to an increase in overall image noise.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Parallel Processing}{43}{subsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{43}{section*.53}}
\citation{cuda_c_guide}
\citation{cuda_c_guide}
\citation{global_vs_shared}
\citation{dahm2017learning}
\citation{muller2018neural}
\citation{alg:neural_q_path_tracer}
\citation{global_data_transfer}
\citation{muller2018neural}
\citation{keller2019integral}
\citation{tensor_cores}
\citation{georgiev2018arnold}
\citation{georgiev2018arnold}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q}{44}{section*.54}}
\citation{dahm2017learning}
\citation{mnih2013playing}
\citation{van2016deep}
\citation{mnih2015human}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{hyperion}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Hyperparameters}{45}{section.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{Expected Sarsa}{45}{section*.55}}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q}{45}{section*.56}}
\citation{dahm2017learning}
\citation{muller2017practical}
\citation{vorba2014line}
\citation{zheng2018learning}
\citation{muller2018neural}
\citation{keller2019integral}
\citation{hermosilla2018deep}
\citation{muller2017practical}
\citation{zheng2018learning}
\citation{dahm2017learning}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{dinh2014nice}
\citation{muller2018neural}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Recent Advancements in Neural Importance Sampling for Monte Carlo Path Tracing}{46}{section.4.7}}
\newlabel{sec:recent_advancements}{{4.7}{46}{Recent Advancements in Neural Importance Sampling for Monte Carlo Path Tracing}{section.4.7}{}}
\citation{keller2015path}
\citation{akenine2018real}
\citation{vorba2014line}
\citation{muller2017practical}
\citation{dahm2017learning}
\citation{dahm2017learning}
\citation{mnih2013playing}
\citation{kajiya1986rendering}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{47}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{47}{Conclusions}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Wider Motivation and the Problem}{47}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Summary of Contributions}{47}{section.5.2}}
\citation{keller2019integral}
\citation{muller2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Discussion}{48}{section.5.3}}
\citation{keller2019integral}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{zheng2018learning}
\citation{keller2019integral}
\citation{muller2018neural}
\citation{muller2018neural}
\citation{nvidia_turing_architecture_whitepaper_2018}
\citation{tensor_cores}
\citation{boulos2005notes}
\citation{georgiev2018arnold}
\citation{christensen2018renderman}
\citation{bako2017kernel}
\citation{chaitanya2017interactive}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Future Work}{49}{section.5.4}}
\@writefile{toc}{\contentsline {subsubsection}{Scaling up Neural-Q}{49}{section*.61}}
\citation{bishop2006pattern}
\citation{muller2018neural}
\citation{van2016deep}
\citation{sutton2011reinforcement}
\citation{keller2019integral}
\citation{lillicrap2015continuous}
\citation{mnih2016asynchronous}
\@writefile{toc}{\contentsline {subsubsection}{Investigating the relationship between scene geometry and ANN requirements}{50}{section*.62}}
\@writefile{toc}{\contentsline {subsubsection}{Neural-Q versus Disney's Neural Path Guider}{50}{section*.63}}
\@writefile{toc}{\contentsline {subsubsection}{Further investigate Deep Reinforcement learning techniques}{50}{section*.64}}
\bibdata{dissertation}
\bibcite{glm}{1}
\bibcite{sdl2}{2}
\bibcite{tensorflow2015-whitepaper}{3}
\bibcite{akenine2018real}{4}
\bibcite{alerstam2008parallel}{5}
\bibcite{accelerated_ray_tracing}{6}
\bibcite{maya}{7}
\bibcite{bako2017kernel}{8}
\bibcite{bashford2012significance}{9}
\bibcite{bellman1966dynamic}{10}
\bibcite{bentley1975multidimensional}{11}
\bibcite{bishop2006pattern}{12}
\bibcite{bloomberg.com}{13}
\bibcite{boulos2005notes}{14}
\bibcite{chaitanya2017interactive}{15}
\bibcite{christensen2018renderman}{16}
\bibcite{christensen2016path}{17}
\bibcite{cline2008table}{18}
\bibcite{crockett1995parallel}{19}
\bibcite{dahm2017learning}{20}
\bibcite{devroye2006nonuniform}{21}
\bibcite{dinh2014nice}{22}
\bibcite{dutre2004state}{23}
\bibcite{fatahalian2009data}{24}
\bibcite{embarissingly_parallelizable}{25}
\bibcite{georgiev2018arnold}{26}
\bibcite{glassner2014principles}{27}
\bibcite{goral1984modeling}{28}
\bibcite{greger1998irradiance}{29}
\bibcite{stanford_graphics}{30}
\bibcite{global_data_transfer}{31}
\bibcite{global_vs_shared}{32}
\bibcite{hermosilla2018deep}{33}
\bibcite{jensen1996global}{34}
\bibcite{kajiya1986rendering}{35}
\bibcite{keller2019integral}{36}
\bibcite{keller2016path}{37}
\bibcite{keller2015path}{38}
\bibcite{kingma2014adam}{39}
\bibcite{konidaris2011value}{40}
\bibcite{krivanek2014recent}{41}
\bibcite{lecun2015deep}{42}
\bibcite{lillicrap2015continuous}{43}
\bibcite{mnih2016asynchronous}{44}
\bibcite{mnih2013playing}{45}
\bibcite{mnih2015human}{46}
\bibcite{morokoff1995quasi}{47}
\bibcite{muller2017practical}{48}
\bibcite{muller2018neural}{49}
\bibcite{nair2010rectified}{50}
\bibcite{dynet}{51}
\bibcite{cuda}{52}
\bibcite{tensor_cores}{53}
\bibcite{cuda_c_guide}{54}
\bibcite{nvidia_turing_architecture_whitepaper_2018}{55}
\bibcite{pan2006virtual}{56}
\bibcite{pegoraro2008towards}{57}
\bibcite{ramamoorthi2012theory}{58}
\bibcite{raynal2012concurrent}{59}
\bibcite{ren2013global}{60}
\bibcite{devastator}{61}
\bibcite{scratchapixel_2015}{62}
\bibcite{shirley1994notes}{63}
\bibcite{hyperion}{64}
\bibcite{sutton1996generalization}{65}
\bibcite{sutton2011reinforcement}{66}
\bibcite{uther1998tree}{67}
\bibcite{exploration_vs_exploitation}{68}
\bibcite{deep_rl_function_approx}{69}
\bibcite{introToRL}{70}
\bibcite{mdp_dynamic_prog}{71}
\bibcite{model_free_prediction}{72}
\bibcite{van2016deep}{73}
\bibcite{vorba2014line}{74}
\bibcite{wald2001state}{75}
\bibcite{normals}{76}
\bibcite{whitted2005improved}{77}
\bibcite{zheng2018learning}{78}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{55}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appx:images}{{A}{55}{Appendix}{appendix.A}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces 128 SPP renders for the Shelter scene.\relax }}{55}{figure.caption.66}}
\newlabel{fig:shelter}{{A.1}{55}{128 SPP renders for the Shelter scene.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces 128 SPP renders for the Cornell Box scene.\relax }}{56}{figure.caption.67}}
\newlabel{fig:cornell}{{A.2}{56}{128 SPP renders for the Cornell Box scene.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces 128 SPP renders for the Complex Pillars scene.\relax }}{57}{figure.caption.68}}
\newlabel{fig:complex_light}{{A.3}{57}{128 SPP renders for the Complex Pillars scene.\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces 128 SPP renders for the Door Room scene.\relax }}{58}{figure.caption.69}}
\newlabel{fig:door_room}{{A.4}{58}{128 SPP renders for the Door Room scene.\relax }{figure.caption.69}{}}
